{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electrolyzer Data Exploratoy Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "    \n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Input, Conv1D, Flatten, Dense, Reshape, Conv1DTranspose\n",
    "from keras.models import Model\n",
    "from keras.models import clone_and_build_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from helper_function import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskPredModel(InputFeature):\n",
    "    # Encoder part\n",
    "    input_layer = Input(shape=(InputFeature.shape[1], InputFeature.shape[2]))\n",
    "    x = Conv1D(32, 3, activation='relu', padding='same')(input_layer)\n",
    "    x = Conv1D(32, 3, activation='relu', padding='same')(x)\n",
    "    shape1=x.get_shape()[1:]\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    ##### First head for masked ouput prediction (linear)\n",
    "    d1 = Reshape(shape1)(x)\n",
    "    d1 = Conv1DTranspose(32, 3, strides=1, activation='relu', padding='same')(d1)\n",
    "    d1 = Conv1DTranspose(6, 3, strides=1, activation='linear', padding='same')(d1)\n",
    "\n",
    "    ##### Second head for masked ouput prediction (boolean)\n",
    "    d2 = Reshape(shape1)(x)\n",
    "    d2 = Conv1DTranspose(32, 3, strides=1, activation='relu', padding='same')(d2)\n",
    "    d2 = Conv1DTranspose(6, 3, strides=1, activation='sigmoid', padding='same')(d2)\n",
    "    \n",
    "    # Define the autoencoder model\n",
    "    autoencoder = Model(input_layer, [d1,d2])\n",
    "\n",
    "    # Compile the model with mean squared error (MSE) loss function and Adam optimizer\n",
    "    autoencoder.compile(loss=['mean_squared_error','binary_crossentropy'], optimizer='adam')\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"Electrolyzer_faults.csv\")\n",
    "X,Y,Z = sliding_window(data, window_size=40, stride=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc= StandardScaler()\n",
    "X_sc=sc.fit_transform(X.reshape(-1,X.shape[2])).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, z_train, z_test = train_test_split(X_sc, Z, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTA-2: Masked prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(X, mask_percentage=0.10):\n",
    "    X_masked = np.zeros(X.shape)\n",
    "    Mask = np.zeros(X.shape)\n",
    "    for i,x in enumerate(X):\n",
    "        binary_mask = np.random.choice([1, 0], size=x.shape, p=[1 - mask_percentage, mask_percentage])\n",
    "        x_masked = x * binary_mask\n",
    "        X_masked[i] = x_masked\n",
    "        Mask[i] = binary_mask\n",
    "    return X_masked, Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mask,train_mask = mask_data(x_train, mask_percentage=0.10)\n",
    "x_test_mask,test_mask = mask_data(x_test, mask_percentage=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining using hybrid method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "29/29 [==============================] - 3s 19ms/step - loss: 0.4987 - binary_accuracy: 0.8334 - val_loss: 0.2968 - val_binary_accuracy: 0.9269\n",
      "Epoch 2/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.2278 - binary_accuracy: 0.9308 - val_loss: 0.1487 - val_binary_accuracy: 0.9502\n",
      "Epoch 3/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.1362 - binary_accuracy: 0.9534 - val_loss: 0.1163 - val_binary_accuracy: 0.9584\n",
      "Epoch 4/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.1108 - binary_accuracy: 0.9597 - val_loss: 0.0987 - val_binary_accuracy: 0.9616\n",
      "Epoch 5/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0969 - binary_accuracy: 0.9638 - val_loss: 0.0944 - val_binary_accuracy: 0.9638\n",
      "Epoch 6/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0889 - binary_accuracy: 0.9664 - val_loss: 0.0870 - val_binary_accuracy: 0.9665\n",
      "Epoch 7/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0844 - binary_accuracy: 0.9688 - val_loss: 0.0797 - val_binary_accuracy: 0.9682\n",
      "Epoch 8/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0795 - binary_accuracy: 0.9709 - val_loss: 0.0780 - val_binary_accuracy: 0.9698\n",
      "Epoch 9/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0776 - binary_accuracy: 0.9710 - val_loss: 0.0751 - val_binary_accuracy: 0.9700\n",
      "Epoch 10/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0744 - binary_accuracy: 0.9718 - val_loss: 0.0711 - val_binary_accuracy: 0.9720\n",
      "Epoch 11/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0713 - binary_accuracy: 0.9727 - val_loss: 0.0711 - val_binary_accuracy: 0.9716\n",
      "Epoch 12/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0709 - binary_accuracy: 0.9727 - val_loss: 0.0701 - val_binary_accuracy: 0.9738\n",
      "Epoch 13/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0684 - binary_accuracy: 0.9737 - val_loss: 0.0667 - val_binary_accuracy: 0.9738\n",
      "Epoch 14/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0655 - binary_accuracy: 0.9755 - val_loss: 0.0657 - val_binary_accuracy: 0.9739\n",
      "Epoch 15/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0640 - binary_accuracy: 0.9756 - val_loss: 0.0644 - val_binary_accuracy: 0.9741\n",
      "Epoch 16/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0624 - binary_accuracy: 0.9763 - val_loss: 0.0626 - val_binary_accuracy: 0.9748\n",
      "Epoch 17/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0613 - binary_accuracy: 0.9766 - val_loss: 0.0649 - val_binary_accuracy: 0.9754\n",
      "Epoch 18/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0609 - binary_accuracy: 0.9765 - val_loss: 0.0599 - val_binary_accuracy: 0.9759\n",
      "Epoch 19/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0610 - binary_accuracy: 0.9761 - val_loss: 0.0609 - val_binary_accuracy: 0.9759\n",
      "Epoch 20/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0620 - binary_accuracy: 0.9773 - val_loss: 0.0598 - val_binary_accuracy: 0.9761\n",
      "Epoch 21/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0598 - binary_accuracy: 0.9774 - val_loss: 0.0585 - val_binary_accuracy: 0.9754\n",
      "Epoch 22/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0561 - binary_accuracy: 0.9784 - val_loss: 0.0597 - val_binary_accuracy: 0.9758\n",
      "Epoch 23/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0566 - binary_accuracy: 0.9780 - val_loss: 0.0568 - val_binary_accuracy: 0.9781\n",
      "Epoch 24/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0549 - binary_accuracy: 0.9788 - val_loss: 0.0556 - val_binary_accuracy: 0.9786\n",
      "Epoch 25/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0542 - binary_accuracy: 0.9788 - val_loss: 0.0584 - val_binary_accuracy: 0.9752\n",
      "Epoch 26/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0543 - binary_accuracy: 0.9791 - val_loss: 0.0579 - val_binary_accuracy: 0.9761\n",
      "Epoch 27/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0525 - binary_accuracy: 0.9788 - val_loss: 0.0623 - val_binary_accuracy: 0.9758\n",
      "Epoch 28/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0547 - binary_accuracy: 0.9786 - val_loss: 0.0656 - val_binary_accuracy: 0.9750\n",
      "Epoch 29/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0548 - binary_accuracy: 0.9800 - val_loss: 0.0557 - val_binary_accuracy: 0.9768\n",
      "Epoch 30/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0514 - binary_accuracy: 0.9798 - val_loss: 0.0523 - val_binary_accuracy: 0.9796\n",
      "Epoch 31/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0507 - binary_accuracy: 0.9809 - val_loss: 0.0535 - val_binary_accuracy: 0.9774\n",
      "Epoch 32/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0496 - binary_accuracy: 0.9809 - val_loss: 0.0524 - val_binary_accuracy: 0.9785\n",
      "Epoch 33/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0508 - binary_accuracy: 0.9812 - val_loss: 0.0563 - val_binary_accuracy: 0.9774\n",
      "Epoch 34/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0513 - binary_accuracy: 0.9806 - val_loss: 0.0632 - val_binary_accuracy: 0.9750\n",
      "Epoch 35/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0491 - binary_accuracy: 0.9815 - val_loss: 0.0513 - val_binary_accuracy: 0.9794\n",
      "Epoch 36/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0461 - binary_accuracy: 0.9824 - val_loss: 0.0502 - val_binary_accuracy: 0.9785\n",
      "Epoch 37/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0466 - binary_accuracy: 0.9830 - val_loss: 0.0515 - val_binary_accuracy: 0.9794\n",
      "Epoch 38/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0483 - binary_accuracy: 0.9826 - val_loss: 0.0517 - val_binary_accuracy: 0.9786\n",
      "Epoch 39/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0463 - binary_accuracy: 0.9827 - val_loss: 0.0520 - val_binary_accuracy: 0.9783\n",
      "Epoch 40/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0447 - binary_accuracy: 0.9842 - val_loss: 0.0553 - val_binary_accuracy: 0.9777\n",
      "Epoch 41/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0453 - binary_accuracy: 0.9830 - val_loss: 0.0486 - val_binary_accuracy: 0.9806\n",
      "Epoch 42/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0448 - binary_accuracy: 0.9844 - val_loss: 0.0472 - val_binary_accuracy: 0.9805\n",
      "Epoch 43/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0434 - binary_accuracy: 0.9846 - val_loss: 0.0465 - val_binary_accuracy: 0.9830\n",
      "Epoch 44/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0419 - binary_accuracy: 0.9850 - val_loss: 0.0477 - val_binary_accuracy: 0.9823\n",
      "Epoch 45/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0417 - binary_accuracy: 0.9850 - val_loss: 0.0479 - val_binary_accuracy: 0.9832\n",
      "Epoch 46/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0407 - binary_accuracy: 0.9853 - val_loss: 0.0498 - val_binary_accuracy: 0.9808\n",
      "Epoch 47/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0408 - binary_accuracy: 0.9865 - val_loss: 0.0477 - val_binary_accuracy: 0.9823\n",
      "Epoch 48/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0403 - binary_accuracy: 0.9859 - val_loss: 0.0471 - val_binary_accuracy: 0.9832\n",
      "Epoch 49/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0415 - binary_accuracy: 0.9852 - val_loss: 0.0452 - val_binary_accuracy: 0.9826\n",
      "Epoch 50/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0397 - binary_accuracy: 0.9862 - val_loss: 0.0450 - val_binary_accuracy: 0.9839\n",
      "Epoch 51/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0401 - binary_accuracy: 0.9864 - val_loss: 0.0499 - val_binary_accuracy: 0.9814\n",
      "Epoch 52/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0415 - binary_accuracy: 0.9854 - val_loss: 0.0487 - val_binary_accuracy: 0.9824\n",
      "Epoch 53/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0409 - binary_accuracy: 0.9858 - val_loss: 0.0453 - val_binary_accuracy: 0.9839\n",
      "Epoch 54/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0378 - binary_accuracy: 0.9874 - val_loss: 0.0453 - val_binary_accuracy: 0.9834\n",
      "Epoch 55/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0370 - binary_accuracy: 0.9873 - val_loss: 0.0448 - val_binary_accuracy: 0.9823\n",
      "Epoch 56/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0359 - binary_accuracy: 0.9883 - val_loss: 0.0439 - val_binary_accuracy: 0.9823\n",
      "Epoch 57/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0358 - binary_accuracy: 0.9883 - val_loss: 0.0447 - val_binary_accuracy: 0.9835\n",
      "Epoch 58/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0363 - binary_accuracy: 0.9880 - val_loss: 0.0506 - val_binary_accuracy: 0.9808\n",
      "Epoch 59/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0394 - binary_accuracy: 0.9860 - val_loss: 0.0454 - val_binary_accuracy: 0.9848\n",
      "Epoch 60/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0372 - binary_accuracy: 0.9870 - val_loss: 0.0435 - val_binary_accuracy: 0.9837\n",
      "Epoch 61/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0383 - binary_accuracy: 0.9872 - val_loss: 0.0422 - val_binary_accuracy: 0.9844\n",
      "Epoch 62/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0368 - binary_accuracy: 0.9880 - val_loss: 0.0435 - val_binary_accuracy: 0.9841\n",
      "Epoch 63/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0347 - binary_accuracy: 0.9886 - val_loss: 0.0440 - val_binary_accuracy: 0.9844\n",
      "Epoch 64/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0364 - binary_accuracy: 0.9883 - val_loss: 0.0434 - val_binary_accuracy: 0.9835\n",
      "Epoch 65/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0345 - binary_accuracy: 0.9888 - val_loss: 0.0442 - val_binary_accuracy: 0.9835\n",
      "Epoch 66/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0368 - binary_accuracy: 0.9870 - val_loss: 0.0503 - val_binary_accuracy: 0.9808\n",
      "Epoch 67/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0353 - binary_accuracy: 0.9882 - val_loss: 0.0430 - val_binary_accuracy: 0.9837\n",
      "Epoch 68/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0346 - binary_accuracy: 0.9888 - val_loss: 0.0453 - val_binary_accuracy: 0.9828\n",
      "Epoch 69/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0336 - binary_accuracy: 0.9893 - val_loss: 0.0429 - val_binary_accuracy: 0.9844\n",
      "Epoch 70/500\n",
      "29/29 [==============================] - 0s 5ms/step - loss: 0.0338 - binary_accuracy: 0.9890 - val_loss: 0.0462 - val_binary_accuracy: 0.9828\n",
      "Epoch 71/500\n",
      "29/29 [==============================] - 0s 6ms/step - loss: 0.0341 - binary_accuracy: 0.9885 - val_loss: 0.0448 - val_binary_accuracy: 0.9841\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "#Pretraining using pseudo labels\n",
    "psuedo_label_model = DeepLearningModel(x_train, z_train,last_layer_activation='sigmoid',loss_fn='binary_crossentropy')\n",
    "# Define early stopping callback to monitor validation loss and stop if it doesn't improve for 5 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with 20 epochs and batch size of 32, using the early stopping callback\n",
    "history = psuedo_label_model.fit(x_train, z_train, epochs=500, batch_size=128, validation_data=(x_test, z_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "27/27 [==============================] - 2s 26ms/step - loss: 1.2343 - conv1d_transpose_1_loss: 0.6579 - conv1d_transpose_3_loss: 0.5764 - val_loss: 0.7412 - val_conv1d_transpose_1_loss: 0.3226 - val_conv1d_transpose_3_loss: 0.4186\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.5626 - conv1d_transpose_1_loss: 0.1722 - conv1d_transpose_3_loss: 0.3904 - val_loss: 0.4708 - val_conv1d_transpose_1_loss: 0.1000 - val_conv1d_transpose_3_loss: 0.3708\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.4477 - conv1d_transpose_1_loss: 0.0853 - conv1d_transpose_3_loss: 0.3624 - val_loss: 0.4294 - val_conv1d_transpose_1_loss: 0.0775 - val_conv1d_transpose_3_loss: 0.3519\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.4194 - conv1d_transpose_1_loss: 0.0719 - conv1d_transpose_3_loss: 0.3475 - val_loss: 0.4094 - val_conv1d_transpose_1_loss: 0.0693 - val_conv1d_transpose_3_loss: 0.3401\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.4030 - conv1d_transpose_1_loss: 0.0656 - conv1d_transpose_3_loss: 0.3375 - val_loss: 0.3971 - val_conv1d_transpose_1_loss: 0.0646 - val_conv1d_transpose_3_loss: 0.3325\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.3929 - conv1d_transpose_1_loss: 0.0614 - conv1d_transpose_3_loss: 0.3315 - val_loss: 0.3884 - val_conv1d_transpose_1_loss: 0.0605 - val_conv1d_transpose_3_loss: 0.3279\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.3856 - conv1d_transpose_1_loss: 0.0580 - conv1d_transpose_3_loss: 0.3276 - val_loss: 0.3822 - val_conv1d_transpose_1_loss: 0.0576 - val_conv1d_transpose_3_loss: 0.3246\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3799 - conv1d_transpose_1_loss: 0.0551 - conv1d_transpose_3_loss: 0.3247 - val_loss: 0.3768 - val_conv1d_transpose_1_loss: 0.0548 - val_conv1d_transpose_3_loss: 0.3220\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3750 - conv1d_transpose_1_loss: 0.0526 - conv1d_transpose_3_loss: 0.3224 - val_loss: 0.3726 - val_conv1d_transpose_1_loss: 0.0528 - val_conv1d_transpose_3_loss: 0.3198\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3709 - conv1d_transpose_1_loss: 0.0506 - conv1d_transpose_3_loss: 0.3203 - val_loss: 0.3684 - val_conv1d_transpose_1_loss: 0.0507 - val_conv1d_transpose_3_loss: 0.3178\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3669 - conv1d_transpose_1_loss: 0.0487 - conv1d_transpose_3_loss: 0.3182 - val_loss: 0.3644 - val_conv1d_transpose_1_loss: 0.0486 - val_conv1d_transpose_3_loss: 0.3157\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.3630 - conv1d_transpose_1_loss: 0.0469 - conv1d_transpose_3_loss: 0.3161 - val_loss: 0.3605 - val_conv1d_transpose_1_loss: 0.0469 - val_conv1d_transpose_3_loss: 0.3136\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.3590 - conv1d_transpose_1_loss: 0.0451 - conv1d_transpose_3_loss: 0.3139 - val_loss: 0.3570 - val_conv1d_transpose_1_loss: 0.0456 - val_conv1d_transpose_3_loss: 0.3114\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3552 - conv1d_transpose_1_loss: 0.0437 - conv1d_transpose_3_loss: 0.3115 - val_loss: 0.3528 - val_conv1d_transpose_1_loss: 0.0439 - val_conv1d_transpose_3_loss: 0.3089\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 0.3510 - conv1d_transpose_1_loss: 0.0422 - conv1d_transpose_3_loss: 0.3088 - val_loss: 0.3487 - val_conv1d_transpose_1_loss: 0.0426 - val_conv1d_transpose_3_loss: 0.3061\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3465 - conv1d_transpose_1_loss: 0.0409 - conv1d_transpose_3_loss: 0.3056 - val_loss: 0.3440 - val_conv1d_transpose_1_loss: 0.0411 - val_conv1d_transpose_3_loss: 0.3029\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.3415 - conv1d_transpose_1_loss: 0.0398 - conv1d_transpose_3_loss: 0.3017 - val_loss: 0.3390 - val_conv1d_transpose_1_loss: 0.0403 - val_conv1d_transpose_3_loss: 0.2987\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3359 - conv1d_transpose_1_loss: 0.0389 - conv1d_transpose_3_loss: 0.2971 - val_loss: 0.3324 - val_conv1d_transpose_1_loss: 0.0392 - val_conv1d_transpose_3_loss: 0.2932\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3287 - conv1d_transpose_1_loss: 0.0377 - conv1d_transpose_3_loss: 0.2910 - val_loss: 0.3247 - val_conv1d_transpose_1_loss: 0.0381 - val_conv1d_transpose_3_loss: 0.2866\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3210 - conv1d_transpose_1_loss: 0.0371 - conv1d_transpose_3_loss: 0.2838 - val_loss: 0.3170 - val_conv1d_transpose_1_loss: 0.0377 - val_conv1d_transpose_3_loss: 0.2792\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3127 - conv1d_transpose_1_loss: 0.0364 - conv1d_transpose_3_loss: 0.2763 - val_loss: 0.3086 - val_conv1d_transpose_1_loss: 0.0369 - val_conv1d_transpose_3_loss: 0.2717\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.3043 - conv1d_transpose_1_loss: 0.0357 - conv1d_transpose_3_loss: 0.2686 - val_loss: 0.3005 - val_conv1d_transpose_1_loss: 0.0361 - val_conv1d_transpose_3_loss: 0.2645\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2962 - conv1d_transpose_1_loss: 0.0351 - conv1d_transpose_3_loss: 0.2611 - val_loss: 0.2926 - val_conv1d_transpose_1_loss: 0.0355 - val_conv1d_transpose_3_loss: 0.2571\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.2881 - conv1d_transpose_1_loss: 0.0342 - conv1d_transpose_3_loss: 0.2539 - val_loss: 0.2851 - val_conv1d_transpose_1_loss: 0.0351 - val_conv1d_transpose_3_loss: 0.2500\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2808 - conv1d_transpose_1_loss: 0.0336 - conv1d_transpose_3_loss: 0.2472 - val_loss: 0.2778 - val_conv1d_transpose_1_loss: 0.0342 - val_conv1d_transpose_3_loss: 0.2436\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2746 - conv1d_transpose_1_loss: 0.0333 - conv1d_transpose_3_loss: 0.2413 - val_loss: 0.2720 - val_conv1d_transpose_1_loss: 0.0339 - val_conv1d_transpose_3_loss: 0.2381\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2691 - conv1d_transpose_1_loss: 0.0329 - conv1d_transpose_3_loss: 0.2362 - val_loss: 0.2667 - val_conv1d_transpose_1_loss: 0.0335 - val_conv1d_transpose_3_loss: 0.2331\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2638 - conv1d_transpose_1_loss: 0.0322 - conv1d_transpose_3_loss: 0.2316 - val_loss: 0.2618 - val_conv1d_transpose_1_loss: 0.0328 - val_conv1d_transpose_3_loss: 0.2291\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2594 - conv1d_transpose_1_loss: 0.0317 - conv1d_transpose_3_loss: 0.2277 - val_loss: 0.2572 - val_conv1d_transpose_1_loss: 0.0318 - val_conv1d_transpose_3_loss: 0.2254\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2549 - conv1d_transpose_1_loss: 0.0307 - conv1d_transpose_3_loss: 0.2242 - val_loss: 0.2536 - val_conv1d_transpose_1_loss: 0.0317 - val_conv1d_transpose_3_loss: 0.2220\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2514 - conv1d_transpose_1_loss: 0.0303 - conv1d_transpose_3_loss: 0.2210 - val_loss: 0.2505 - val_conv1d_transpose_1_loss: 0.0311 - val_conv1d_transpose_3_loss: 0.2194\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 0.2480 - conv1d_transpose_1_loss: 0.0296 - conv1d_transpose_3_loss: 0.2184 - val_loss: 0.2474 - val_conv1d_transpose_1_loss: 0.0305 - val_conv1d_transpose_3_loss: 0.2169\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2453 - conv1d_transpose_1_loss: 0.0293 - conv1d_transpose_3_loss: 0.2160 - val_loss: 0.2451 - val_conv1d_transpose_1_loss: 0.0304 - val_conv1d_transpose_3_loss: 0.2146\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2433 - conv1d_transpose_1_loss: 0.0294 - conv1d_transpose_3_loss: 0.2139 - val_loss: 0.2431 - val_conv1d_transpose_1_loss: 0.0303 - val_conv1d_transpose_3_loss: 0.2127\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2409 - conv1d_transpose_1_loss: 0.0288 - conv1d_transpose_3_loss: 0.2121 - val_loss: 0.2403 - val_conv1d_transpose_1_loss: 0.0292 - val_conv1d_transpose_3_loss: 0.2110\n",
      "Epoch 36/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.2386 - conv1d_transpose_1_loss: 0.0280 - conv1d_transpose_3_loss: 0.2106 - val_loss: 0.2391 - val_conv1d_transpose_1_loss: 0.0296 - val_conv1d_transpose_3_loss: 0.2095\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2364 - conv1d_transpose_1_loss: 0.0275 - conv1d_transpose_3_loss: 0.2089 - val_loss: 0.2368 - val_conv1d_transpose_1_loss: 0.0286 - val_conv1d_transpose_3_loss: 0.2082\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2344 - conv1d_transpose_1_loss: 0.0270 - conv1d_transpose_3_loss: 0.2074 - val_loss: 0.2345 - val_conv1d_transpose_1_loss: 0.0279 - val_conv1d_transpose_3_loss: 0.2066\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 0.2328 - conv1d_transpose_1_loss: 0.0267 - conv1d_transpose_3_loss: 0.2061 - val_loss: 0.2330 - val_conv1d_transpose_1_loss: 0.0275 - val_conv1d_transpose_3_loss: 0.2055\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2314 - conv1d_transpose_1_loss: 0.0262 - conv1d_transpose_3_loss: 0.2052 - val_loss: 0.2319 - val_conv1d_transpose_1_loss: 0.0275 - val_conv1d_transpose_3_loss: 0.2044\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.2300 - conv1d_transpose_1_loss: 0.0260 - conv1d_transpose_3_loss: 0.2040 - val_loss: 0.2303 - val_conv1d_transpose_1_loss: 0.0270 - val_conv1d_transpose_3_loss: 0.2033\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2283 - conv1d_transpose_1_loss: 0.0255 - conv1d_transpose_3_loss: 0.2027 - val_loss: 0.2290 - val_conv1d_transpose_1_loss: 0.0266 - val_conv1d_transpose_3_loss: 0.2024\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2270 - conv1d_transpose_1_loss: 0.0253 - conv1d_transpose_3_loss: 0.2018 - val_loss: 0.2281 - val_conv1d_transpose_1_loss: 0.0266 - val_conv1d_transpose_3_loss: 0.2016\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2259 - conv1d_transpose_1_loss: 0.0250 - conv1d_transpose_3_loss: 0.2009 - val_loss: 0.2280 - val_conv1d_transpose_1_loss: 0.0270 - val_conv1d_transpose_3_loss: 0.2010\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2251 - conv1d_transpose_1_loss: 0.0250 - conv1d_transpose_3_loss: 0.2001 - val_loss: 0.2264 - val_conv1d_transpose_1_loss: 0.0266 - val_conv1d_transpose_3_loss: 0.1998\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2237 - conv1d_transpose_1_loss: 0.0245 - conv1d_transpose_3_loss: 0.1992 - val_loss: 0.2256 - val_conv1d_transpose_1_loss: 0.0264 - val_conv1d_transpose_3_loss: 0.1992\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2228 - conv1d_transpose_1_loss: 0.0247 - conv1d_transpose_3_loss: 0.1982 - val_loss: 0.2239 - val_conv1d_transpose_1_loss: 0.0254 - val_conv1d_transpose_3_loss: 0.1985\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 0.2216 - conv1d_transpose_1_loss: 0.0242 - conv1d_transpose_3_loss: 0.1974 - val_loss: 0.2231 - val_conv1d_transpose_1_loss: 0.0255 - val_conv1d_transpose_3_loss: 0.1976\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.2206 - conv1d_transpose_1_loss: 0.0238 - conv1d_transpose_3_loss: 0.1967 - val_loss: 0.2237 - val_conv1d_transpose_1_loss: 0.0262 - val_conv1d_transpose_3_loss: 0.1974\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2205 - conv1d_transpose_1_loss: 0.0244 - conv1d_transpose_3_loss: 0.1961 - val_loss: 0.2217 - val_conv1d_transpose_1_loss: 0.0255 - val_conv1d_transpose_3_loss: 0.1962\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2186 - conv1d_transpose_1_loss: 0.0232 - conv1d_transpose_3_loss: 0.1953 - val_loss: 0.2200 - val_conv1d_transpose_1_loss: 0.0244 - val_conv1d_transpose_3_loss: 0.1956\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2176 - conv1d_transpose_1_loss: 0.0228 - conv1d_transpose_3_loss: 0.1948 - val_loss: 0.2189 - val_conv1d_transpose_1_loss: 0.0242 - val_conv1d_transpose_3_loss: 0.1947\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.2166 - conv1d_transpose_1_loss: 0.0227 - conv1d_transpose_3_loss: 0.1939 - val_loss: 0.2184 - val_conv1d_transpose_1_loss: 0.0241 - val_conv1d_transpose_3_loss: 0.1943\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.2162 - conv1d_transpose_1_loss: 0.0229 - conv1d_transpose_3_loss: 0.1934 - val_loss: 0.2181 - val_conv1d_transpose_1_loss: 0.0244 - val_conv1d_transpose_3_loss: 0.1937\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.2158 - conv1d_transpose_1_loss: 0.0232 - conv1d_transpose_3_loss: 0.1925 - val_loss: 0.2169 - val_conv1d_transpose_1_loss: 0.0236 - val_conv1d_transpose_3_loss: 0.1933\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2139 - conv1d_transpose_1_loss: 0.0220 - conv1d_transpose_3_loss: 0.1919 - val_loss: 0.2159 - val_conv1d_transpose_1_loss: 0.0234 - val_conv1d_transpose_3_loss: 0.1925\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.2131 - conv1d_transpose_1_loss: 0.0219 - conv1d_transpose_3_loss: 0.1912 - val_loss: 0.2153 - val_conv1d_transpose_1_loss: 0.0233 - val_conv1d_transpose_3_loss: 0.1920\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2124 - conv1d_transpose_1_loss: 0.0217 - conv1d_transpose_3_loss: 0.1907 - val_loss: 0.2149 - val_conv1d_transpose_1_loss: 0.0235 - val_conv1d_transpose_3_loss: 0.1914\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2118 - conv1d_transpose_1_loss: 0.0215 - conv1d_transpose_3_loss: 0.1903 - val_loss: 0.2140 - val_conv1d_transpose_1_loss: 0.0227 - val_conv1d_transpose_3_loss: 0.1913\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2112 - conv1d_transpose_1_loss: 0.0213 - conv1d_transpose_3_loss: 0.1899 - val_loss: 0.2128 - val_conv1d_transpose_1_loss: 0.0228 - val_conv1d_transpose_3_loss: 0.1900\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2099 - conv1d_transpose_1_loss: 0.0210 - conv1d_transpose_3_loss: 0.1888 - val_loss: 0.2118 - val_conv1d_transpose_1_loss: 0.0223 - val_conv1d_transpose_3_loss: 0.1896\n",
      "Epoch 62/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2091 - conv1d_transpose_1_loss: 0.0208 - conv1d_transpose_3_loss: 0.1883 - val_loss: 0.2110 - val_conv1d_transpose_1_loss: 0.0221 - val_conv1d_transpose_3_loss: 0.1889\n",
      "Epoch 63/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2084 - conv1d_transpose_1_loss: 0.0209 - conv1d_transpose_3_loss: 0.1875 - val_loss: 0.2104 - val_conv1d_transpose_1_loss: 0.0225 - val_conv1d_transpose_3_loss: 0.1879\n",
      "Epoch 64/200\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.2080 - conv1d_transpose_1_loss: 0.0210 - conv1d_transpose_3_loss: 0.1870 - val_loss: 0.2097 - val_conv1d_transpose_1_loss: 0.0222 - val_conv1d_transpose_3_loss: 0.1875\n",
      "Epoch 65/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.2069 - conv1d_transpose_1_loss: 0.0206 - conv1d_transpose_3_loss: 0.1863 - val_loss: 0.2083 - val_conv1d_transpose_1_loss: 0.0217 - val_conv1d_transpose_3_loss: 0.1866\n",
      "Epoch 66/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2063 - conv1d_transpose_1_loss: 0.0206 - conv1d_transpose_3_loss: 0.1856 - val_loss: 0.2083 - val_conv1d_transpose_1_loss: 0.0221 - val_conv1d_transpose_3_loss: 0.1862\n",
      "Epoch 67/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2055 - conv1d_transpose_1_loss: 0.0205 - conv1d_transpose_3_loss: 0.1850 - val_loss: 0.2072 - val_conv1d_transpose_1_loss: 0.0216 - val_conv1d_transpose_3_loss: 0.1856\n",
      "Epoch 68/200\n",
      "27/27 [==============================] - 0s 16ms/step - loss: 0.2047 - conv1d_transpose_1_loss: 0.0203 - conv1d_transpose_3_loss: 0.1844 - val_loss: 0.2069 - val_conv1d_transpose_1_loss: 0.0219 - val_conv1d_transpose_3_loss: 0.1850\n",
      "Epoch 69/200\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.2038 - conv1d_transpose_1_loss: 0.0201 - conv1d_transpose_3_loss: 0.1837 - val_loss: 0.2052 - val_conv1d_transpose_1_loss: 0.0210 - val_conv1d_transpose_3_loss: 0.1842\n",
      "Epoch 70/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.2026 - conv1d_transpose_1_loss: 0.0198 - conv1d_transpose_3_loss: 0.1829 - val_loss: 0.2046 - val_conv1d_transpose_1_loss: 0.0212 - val_conv1d_transpose_3_loss: 0.1834\n",
      "Epoch 71/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.2017 - conv1d_transpose_1_loss: 0.0196 - conv1d_transpose_3_loss: 0.1820 - val_loss: 0.2040 - val_conv1d_transpose_1_loss: 0.0209 - val_conv1d_transpose_3_loss: 0.1831\n",
      "Epoch 72/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.2011 - conv1d_transpose_1_loss: 0.0196 - conv1d_transpose_3_loss: 0.1815 - val_loss: 0.2027 - val_conv1d_transpose_1_loss: 0.0209 - val_conv1d_transpose_3_loss: 0.1819\n",
      "Epoch 73/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1998 - conv1d_transpose_1_loss: 0.0192 - conv1d_transpose_3_loss: 0.1805 - val_loss: 0.2020 - val_conv1d_transpose_1_loss: 0.0209 - val_conv1d_transpose_3_loss: 0.1810\n",
      "Epoch 74/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1990 - conv1d_transpose_1_loss: 0.0193 - conv1d_transpose_3_loss: 0.1797 - val_loss: 0.2014 - val_conv1d_transpose_1_loss: 0.0209 - val_conv1d_transpose_3_loss: 0.1805\n",
      "Epoch 75/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1982 - conv1d_transpose_1_loss: 0.0194 - conv1d_transpose_3_loss: 0.1788 - val_loss: 0.2001 - val_conv1d_transpose_1_loss: 0.0207 - val_conv1d_transpose_3_loss: 0.1793\n",
      "Epoch 76/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1973 - conv1d_transpose_1_loss: 0.0194 - conv1d_transpose_3_loss: 0.1779 - val_loss: 0.1989 - val_conv1d_transpose_1_loss: 0.0209 - val_conv1d_transpose_3_loss: 0.1780\n",
      "Epoch 77/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1957 - conv1d_transpose_1_loss: 0.0188 - conv1d_transpose_3_loss: 0.1769 - val_loss: 0.1978 - val_conv1d_transpose_1_loss: 0.0204 - val_conv1d_transpose_3_loss: 0.1775\n",
      "Epoch 78/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1946 - conv1d_transpose_1_loss: 0.0187 - conv1d_transpose_3_loss: 0.1759 - val_loss: 0.1979 - val_conv1d_transpose_1_loss: 0.0212 - val_conv1d_transpose_3_loss: 0.1767\n",
      "Epoch 79/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1940 - conv1d_transpose_1_loss: 0.0191 - conv1d_transpose_3_loss: 0.1749 - val_loss: 0.1961 - val_conv1d_transpose_1_loss: 0.0207 - val_conv1d_transpose_3_loss: 0.1753\n",
      "Epoch 80/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1931 - conv1d_transpose_1_loss: 0.0193 - conv1d_transpose_3_loss: 0.1737 - val_loss: 0.1949 - val_conv1d_transpose_1_loss: 0.0207 - val_conv1d_transpose_3_loss: 0.1743\n",
      "Epoch 81/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1912 - conv1d_transpose_1_loss: 0.0187 - conv1d_transpose_3_loss: 0.1726 - val_loss: 0.1924 - val_conv1d_transpose_1_loss: 0.0199 - val_conv1d_transpose_3_loss: 0.1725\n",
      "Epoch 82/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1897 - conv1d_transpose_1_loss: 0.0186 - conv1d_transpose_3_loss: 0.1711 - val_loss: 0.1913 - val_conv1d_transpose_1_loss: 0.0206 - val_conv1d_transpose_3_loss: 0.1707\n",
      "Epoch 83/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1885 - conv1d_transpose_1_loss: 0.0191 - conv1d_transpose_3_loss: 0.1695 - val_loss: 0.1903 - val_conv1d_transpose_1_loss: 0.0209 - val_conv1d_transpose_3_loss: 0.1694\n",
      "Epoch 84/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1877 - conv1d_transpose_1_loss: 0.0192 - conv1d_transpose_3_loss: 0.1685 - val_loss: 0.1891 - val_conv1d_transpose_1_loss: 0.0204 - val_conv1d_transpose_3_loss: 0.1686\n",
      "Epoch 85/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1855 - conv1d_transpose_1_loss: 0.0185 - conv1d_transpose_3_loss: 0.1670 - val_loss: 0.1863 - val_conv1d_transpose_1_loss: 0.0196 - val_conv1d_transpose_3_loss: 0.1667\n",
      "Epoch 86/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1837 - conv1d_transpose_1_loss: 0.0181 - conv1d_transpose_3_loss: 0.1656 - val_loss: 0.1855 - val_conv1d_transpose_1_loss: 0.0198 - val_conv1d_transpose_3_loss: 0.1657\n",
      "Epoch 87/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1824 - conv1d_transpose_1_loss: 0.0182 - conv1d_transpose_3_loss: 0.1642 - val_loss: 0.1844 - val_conv1d_transpose_1_loss: 0.0198 - val_conv1d_transpose_3_loss: 0.1645\n",
      "Epoch 88/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1807 - conv1d_transpose_1_loss: 0.0179 - conv1d_transpose_3_loss: 0.1628 - val_loss: 0.1821 - val_conv1d_transpose_1_loss: 0.0194 - val_conv1d_transpose_3_loss: 0.1627\n",
      "Epoch 89/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1797 - conv1d_transpose_1_loss: 0.0179 - conv1d_transpose_3_loss: 0.1618 - val_loss: 0.1809 - val_conv1d_transpose_1_loss: 0.0192 - val_conv1d_transpose_3_loss: 0.1617\n",
      "Epoch 90/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1782 - conv1d_transpose_1_loss: 0.0177 - conv1d_transpose_3_loss: 0.1605 - val_loss: 0.1810 - val_conv1d_transpose_1_loss: 0.0206 - val_conv1d_transpose_3_loss: 0.1603\n",
      "Epoch 91/200\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 0.1772 - conv1d_transpose_1_loss: 0.0180 - conv1d_transpose_3_loss: 0.1591 - val_loss: 0.1789 - val_conv1d_transpose_1_loss: 0.0199 - val_conv1d_transpose_3_loss: 0.1590\n",
      "Epoch 92/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.1757 - conv1d_transpose_1_loss: 0.0178 - conv1d_transpose_3_loss: 0.1579 - val_loss: 0.1774 - val_conv1d_transpose_1_loss: 0.0193 - val_conv1d_transpose_3_loss: 0.1582\n",
      "Epoch 93/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1753 - conv1d_transpose_1_loss: 0.0182 - conv1d_transpose_3_loss: 0.1571 - val_loss: 0.1774 - val_conv1d_transpose_1_loss: 0.0196 - val_conv1d_transpose_3_loss: 0.1578\n",
      "Epoch 94/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1737 - conv1d_transpose_1_loss: 0.0177 - conv1d_transpose_3_loss: 0.1560 - val_loss: 0.1753 - val_conv1d_transpose_1_loss: 0.0189 - val_conv1d_transpose_3_loss: 0.1564\n",
      "Epoch 95/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1731 - conv1d_transpose_1_loss: 0.0180 - conv1d_transpose_3_loss: 0.1551 - val_loss: 0.1750 - val_conv1d_transpose_1_loss: 0.0198 - val_conv1d_transpose_3_loss: 0.1553\n",
      "Epoch 96/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1715 - conv1d_transpose_1_loss: 0.0176 - conv1d_transpose_3_loss: 0.1539 - val_loss: 0.1744 - val_conv1d_transpose_1_loss: 0.0198 - val_conv1d_transpose_3_loss: 0.1546\n",
      "Epoch 97/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1708 - conv1d_transpose_1_loss: 0.0174 - conv1d_transpose_3_loss: 0.1534 - val_loss: 0.1723 - val_conv1d_transpose_1_loss: 0.0189 - val_conv1d_transpose_3_loss: 0.1534\n",
      "Epoch 98/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1700 - conv1d_transpose_1_loss: 0.0174 - conv1d_transpose_3_loss: 0.1525 - val_loss: 0.1718 - val_conv1d_transpose_1_loss: 0.0188 - val_conv1d_transpose_3_loss: 0.1530\n",
      "Epoch 99/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1694 - conv1d_transpose_1_loss: 0.0175 - conv1d_transpose_3_loss: 0.1519 - val_loss: 0.1714 - val_conv1d_transpose_1_loss: 0.0191 - val_conv1d_transpose_3_loss: 0.1522\n",
      "Epoch 100/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1691 - conv1d_transpose_1_loss: 0.0176 - conv1d_transpose_3_loss: 0.1514 - val_loss: 0.1719 - val_conv1d_transpose_1_loss: 0.0198 - val_conv1d_transpose_3_loss: 0.1521\n",
      "Epoch 101/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1681 - conv1d_transpose_1_loss: 0.0173 - conv1d_transpose_3_loss: 0.1508 - val_loss: 0.1697 - val_conv1d_transpose_1_loss: 0.0188 - val_conv1d_transpose_3_loss: 0.1510\n",
      "Epoch 102/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1669 - conv1d_transpose_1_loss: 0.0167 - conv1d_transpose_3_loss: 0.1502 - val_loss: 0.1688 - val_conv1d_transpose_1_loss: 0.0179 - val_conv1d_transpose_3_loss: 0.1510\n",
      "Epoch 103/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1666 - conv1d_transpose_1_loss: 0.0172 - conv1d_transpose_3_loss: 0.1494 - val_loss: 0.1699 - val_conv1d_transpose_1_loss: 0.0199 - val_conv1d_transpose_3_loss: 0.1500\n",
      "Epoch 104/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1664 - conv1d_transpose_1_loss: 0.0175 - conv1d_transpose_3_loss: 0.1489 - val_loss: 0.1684 - val_conv1d_transpose_1_loss: 0.0183 - val_conv1d_transpose_3_loss: 0.1501\n",
      "Epoch 105/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1652 - conv1d_transpose_1_loss: 0.0168 - conv1d_transpose_3_loss: 0.1484 - val_loss: 0.1670 - val_conv1d_transpose_1_loss: 0.0182 - val_conv1d_transpose_3_loss: 0.1487\n",
      "Epoch 106/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1642 - conv1d_transpose_1_loss: 0.0163 - conv1d_transpose_3_loss: 0.1478 - val_loss: 0.1660 - val_conv1d_transpose_1_loss: 0.0177 - val_conv1d_transpose_3_loss: 0.1483\n",
      "Epoch 107/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1637 - conv1d_transpose_1_loss: 0.0163 - conv1d_transpose_3_loss: 0.1474 - val_loss: 0.1668 - val_conv1d_transpose_1_loss: 0.0179 - val_conv1d_transpose_3_loss: 0.1488\n",
      "Epoch 108/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1637 - conv1d_transpose_1_loss: 0.0167 - conv1d_transpose_3_loss: 0.1471 - val_loss: 0.1663 - val_conv1d_transpose_1_loss: 0.0187 - val_conv1d_transpose_3_loss: 0.1476\n",
      "Epoch 109/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1639 - conv1d_transpose_1_loss: 0.0173 - conv1d_transpose_3_loss: 0.1466 - val_loss: 0.1655 - val_conv1d_transpose_1_loss: 0.0185 - val_conv1d_transpose_3_loss: 0.1470\n",
      "Epoch 110/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1627 - conv1d_transpose_1_loss: 0.0166 - conv1d_transpose_3_loss: 0.1461 - val_loss: 0.1646 - val_conv1d_transpose_1_loss: 0.0178 - val_conv1d_transpose_3_loss: 0.1469\n",
      "Epoch 111/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1624 - conv1d_transpose_1_loss: 0.0166 - conv1d_transpose_3_loss: 0.1458 - val_loss: 0.1658 - val_conv1d_transpose_1_loss: 0.0191 - val_conv1d_transpose_3_loss: 0.1468\n",
      "Epoch 112/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1624 - conv1d_transpose_1_loss: 0.0171 - conv1d_transpose_3_loss: 0.1453 - val_loss: 0.1646 - val_conv1d_transpose_1_loss: 0.0186 - val_conv1d_transpose_3_loss: 0.1460\n",
      "Epoch 113/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1611 - conv1d_transpose_1_loss: 0.0161 - conv1d_transpose_3_loss: 0.1450 - val_loss: 0.1633 - val_conv1d_transpose_1_loss: 0.0177 - val_conv1d_transpose_3_loss: 0.1456\n",
      "Epoch 114/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1605 - conv1d_transpose_1_loss: 0.0159 - conv1d_transpose_3_loss: 0.1447 - val_loss: 0.1631 - val_conv1d_transpose_1_loss: 0.0173 - val_conv1d_transpose_3_loss: 0.1458\n",
      "Epoch 115/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1604 - conv1d_transpose_1_loss: 0.0161 - conv1d_transpose_3_loss: 0.1444 - val_loss: 0.1631 - val_conv1d_transpose_1_loss: 0.0185 - val_conv1d_transpose_3_loss: 0.1446\n",
      "Epoch 116/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1599 - conv1d_transpose_1_loss: 0.0161 - conv1d_transpose_3_loss: 0.1439 - val_loss: 0.1624 - val_conv1d_transpose_1_loss: 0.0180 - val_conv1d_transpose_3_loss: 0.1444\n",
      "Epoch 117/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1595 - conv1d_transpose_1_loss: 0.0159 - conv1d_transpose_3_loss: 0.1436 - val_loss: 0.1609 - val_conv1d_transpose_1_loss: 0.0168 - val_conv1d_transpose_3_loss: 0.1440\n",
      "Epoch 118/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1588 - conv1d_transpose_1_loss: 0.0157 - conv1d_transpose_3_loss: 0.1431 - val_loss: 0.1616 - val_conv1d_transpose_1_loss: 0.0178 - val_conv1d_transpose_3_loss: 0.1438\n",
      "Epoch 119/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1585 - conv1d_transpose_1_loss: 0.0157 - conv1d_transpose_3_loss: 0.1428 - val_loss: 0.1605 - val_conv1d_transpose_1_loss: 0.0171 - val_conv1d_transpose_3_loss: 0.1434\n",
      "Epoch 120/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1579 - conv1d_transpose_1_loss: 0.0155 - conv1d_transpose_3_loss: 0.1423 - val_loss: 0.1604 - val_conv1d_transpose_1_loss: 0.0173 - val_conv1d_transpose_3_loss: 0.1431\n",
      "Epoch 121/200\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.1580 - conv1d_transpose_1_loss: 0.0157 - conv1d_transpose_3_loss: 0.1423 - val_loss: 0.1605 - val_conv1d_transpose_1_loss: 0.0171 - val_conv1d_transpose_3_loss: 0.1435\n",
      "Epoch 122/200\n",
      "27/27 [==============================] - 1s 19ms/step - loss: 0.1581 - conv1d_transpose_1_loss: 0.0159 - conv1d_transpose_3_loss: 0.1422 - val_loss: 0.1599 - val_conv1d_transpose_1_loss: 0.0173 - val_conv1d_transpose_3_loss: 0.1426\n",
      "Epoch 123/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1582 - conv1d_transpose_1_loss: 0.0166 - conv1d_transpose_3_loss: 0.1416 - val_loss: 0.1595 - val_conv1d_transpose_1_loss: 0.0173 - val_conv1d_transpose_3_loss: 0.1422\n",
      "Epoch 124/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.1569 - conv1d_transpose_1_loss: 0.0156 - conv1d_transpose_3_loss: 0.1412 - val_loss: 0.1594 - val_conv1d_transpose_1_loss: 0.0174 - val_conv1d_transpose_3_loss: 0.1420\n",
      "Epoch 125/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1562 - conv1d_transpose_1_loss: 0.0154 - conv1d_transpose_3_loss: 0.1408 - val_loss: 0.1587 - val_conv1d_transpose_1_loss: 0.0169 - val_conv1d_transpose_3_loss: 0.1417\n",
      "Epoch 126/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1568 - conv1d_transpose_1_loss: 0.0164 - conv1d_transpose_3_loss: 0.1404 - val_loss: 0.1617 - val_conv1d_transpose_1_loss: 0.0203 - val_conv1d_transpose_3_loss: 0.1414\n",
      "Epoch 127/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.1565 - conv1d_transpose_1_loss: 0.0164 - conv1d_transpose_3_loss: 0.1401 - val_loss: 0.1580 - val_conv1d_transpose_1_loss: 0.0171 - val_conv1d_transpose_3_loss: 0.1410\n",
      "Epoch 128/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1551 - conv1d_transpose_1_loss: 0.0152 - conv1d_transpose_3_loss: 0.1399 - val_loss: 0.1574 - val_conv1d_transpose_1_loss: 0.0170 - val_conv1d_transpose_3_loss: 0.1405\n",
      "Epoch 129/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1547 - conv1d_transpose_1_loss: 0.0151 - conv1d_transpose_3_loss: 0.1396 - val_loss: 0.1569 - val_conv1d_transpose_1_loss: 0.0165 - val_conv1d_transpose_3_loss: 0.1404\n",
      "Epoch 130/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1546 - conv1d_transpose_1_loss: 0.0150 - conv1d_transpose_3_loss: 0.1395 - val_loss: 0.1567 - val_conv1d_transpose_1_loss: 0.0165 - val_conv1d_transpose_3_loss: 0.1402\n",
      "Epoch 131/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1545 - conv1d_transpose_1_loss: 0.0153 - conv1d_transpose_3_loss: 0.1392 - val_loss: 0.1568 - val_conv1d_transpose_1_loss: 0.0164 - val_conv1d_transpose_3_loss: 0.1404\n",
      "Epoch 132/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1539 - conv1d_transpose_1_loss: 0.0149 - conv1d_transpose_3_loss: 0.1390 - val_loss: 0.1567 - val_conv1d_transpose_1_loss: 0.0164 - val_conv1d_transpose_3_loss: 0.1403\n",
      "Epoch 133/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1557 - conv1d_transpose_1_loss: 0.0170 - conv1d_transpose_3_loss: 0.1387 - val_loss: 0.1600 - val_conv1d_transpose_1_loss: 0.0202 - val_conv1d_transpose_3_loss: 0.1399\n",
      "Epoch 134/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1554 - conv1d_transpose_1_loss: 0.0167 - conv1d_transpose_3_loss: 0.1388 - val_loss: 0.1563 - val_conv1d_transpose_1_loss: 0.0168 - val_conv1d_transpose_3_loss: 0.1395\n",
      "Epoch 135/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1542 - conv1d_transpose_1_loss: 0.0156 - conv1d_transpose_3_loss: 0.1385 - val_loss: 0.1564 - val_conv1d_transpose_1_loss: 0.0173 - val_conv1d_transpose_3_loss: 0.1391\n",
      "Epoch 136/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1533 - conv1d_transpose_1_loss: 0.0151 - conv1d_transpose_3_loss: 0.1382 - val_loss: 0.1561 - val_conv1d_transpose_1_loss: 0.0166 - val_conv1d_transpose_3_loss: 0.1395\n",
      "Epoch 137/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1533 - conv1d_transpose_1_loss: 0.0152 - conv1d_transpose_3_loss: 0.1381 - val_loss: 0.1556 - val_conv1d_transpose_1_loss: 0.0171 - val_conv1d_transpose_3_loss: 0.1385\n",
      "Epoch 138/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1524 - conv1d_transpose_1_loss: 0.0147 - conv1d_transpose_3_loss: 0.1377 - val_loss: 0.1542 - val_conv1d_transpose_1_loss: 0.0160 - val_conv1d_transpose_3_loss: 0.1382\n",
      "Epoch 139/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1519 - conv1d_transpose_1_loss: 0.0145 - conv1d_transpose_3_loss: 0.1375 - val_loss: 0.1545 - val_conv1d_transpose_1_loss: 0.0159 - val_conv1d_transpose_3_loss: 0.1385\n",
      "Epoch 140/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1514 - conv1d_transpose_1_loss: 0.0143 - conv1d_transpose_3_loss: 0.1370 - val_loss: 0.1544 - val_conv1d_transpose_1_loss: 0.0162 - val_conv1d_transpose_3_loss: 0.1382\n",
      "Epoch 141/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1510 - conv1d_transpose_1_loss: 0.0144 - conv1d_transpose_3_loss: 0.1366 - val_loss: 0.1537 - val_conv1d_transpose_1_loss: 0.0160 - val_conv1d_transpose_3_loss: 0.1377\n",
      "Epoch 142/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1510 - conv1d_transpose_1_loss: 0.0144 - conv1d_transpose_3_loss: 0.1366 - val_loss: 0.1542 - val_conv1d_transpose_1_loss: 0.0158 - val_conv1d_transpose_3_loss: 0.1383\n",
      "Epoch 143/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1513 - conv1d_transpose_1_loss: 0.0145 - conv1d_transpose_3_loss: 0.1368 - val_loss: 0.1550 - val_conv1d_transpose_1_loss: 0.0169 - val_conv1d_transpose_3_loss: 0.1381\n",
      "Epoch 144/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1516 - conv1d_transpose_1_loss: 0.0148 - conv1d_transpose_3_loss: 0.1368 - val_loss: 0.1555 - val_conv1d_transpose_1_loss: 0.0175 - val_conv1d_transpose_3_loss: 0.1380\n",
      "Epoch 145/200\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.1521 - conv1d_transpose_1_loss: 0.0155 - conv1d_transpose_3_loss: 0.1366 - val_loss: 0.1547 - val_conv1d_transpose_1_loss: 0.0167 - val_conv1d_transpose_3_loss: 0.1380\n",
      "Epoch 146/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1505 - conv1d_transpose_1_loss: 0.0144 - conv1d_transpose_3_loss: 0.1360 - val_loss: 0.1531 - val_conv1d_transpose_1_loss: 0.0157 - val_conv1d_transpose_3_loss: 0.1374\n",
      "Epoch 147/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1504 - conv1d_transpose_1_loss: 0.0147 - conv1d_transpose_3_loss: 0.1356 - val_loss: 0.1529 - val_conv1d_transpose_1_loss: 0.0159 - val_conv1d_transpose_3_loss: 0.1370\n",
      "Epoch 148/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1499 - conv1d_transpose_1_loss: 0.0142 - conv1d_transpose_3_loss: 0.1357 - val_loss: 0.1521 - val_conv1d_transpose_1_loss: 0.0155 - val_conv1d_transpose_3_loss: 0.1365\n",
      "Epoch 149/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1498 - conv1d_transpose_1_loss: 0.0142 - conv1d_transpose_3_loss: 0.1356 - val_loss: 0.1522 - val_conv1d_transpose_1_loss: 0.0155 - val_conv1d_transpose_3_loss: 0.1366\n",
      "Epoch 150/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1492 - conv1d_transpose_1_loss: 0.0140 - conv1d_transpose_3_loss: 0.1352 - val_loss: 0.1519 - val_conv1d_transpose_1_loss: 0.0156 - val_conv1d_transpose_3_loss: 0.1363\n",
      "Epoch 151/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1486 - conv1d_transpose_1_loss: 0.0139 - conv1d_transpose_3_loss: 0.1347 - val_loss: 0.1512 - val_conv1d_transpose_1_loss: 0.0155 - val_conv1d_transpose_3_loss: 0.1357\n",
      "Epoch 152/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1486 - conv1d_transpose_1_loss: 0.0139 - conv1d_transpose_3_loss: 0.1347 - val_loss: 0.1519 - val_conv1d_transpose_1_loss: 0.0158 - val_conv1d_transpose_3_loss: 0.1361\n",
      "Epoch 153/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1489 - conv1d_transpose_1_loss: 0.0141 - conv1d_transpose_3_loss: 0.1348 - val_loss: 0.1514 - val_conv1d_transpose_1_loss: 0.0154 - val_conv1d_transpose_3_loss: 0.1360\n",
      "Epoch 154/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1485 - conv1d_transpose_1_loss: 0.0141 - conv1d_transpose_3_loss: 0.1344 - val_loss: 0.1510 - val_conv1d_transpose_1_loss: 0.0157 - val_conv1d_transpose_3_loss: 0.1353\n",
      "Epoch 155/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1481 - conv1d_transpose_1_loss: 0.0138 - conv1d_transpose_3_loss: 0.1343 - val_loss: 0.1516 - val_conv1d_transpose_1_loss: 0.0161 - val_conv1d_transpose_3_loss: 0.1354\n",
      "Epoch 156/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1490 - conv1d_transpose_1_loss: 0.0141 - conv1d_transpose_3_loss: 0.1349 - val_loss: 0.1513 - val_conv1d_transpose_1_loss: 0.0162 - val_conv1d_transpose_3_loss: 0.1350\n",
      "Epoch 157/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1484 - conv1d_transpose_1_loss: 0.0144 - conv1d_transpose_3_loss: 0.1340 - val_loss: 0.1511 - val_conv1d_transpose_1_loss: 0.0160 - val_conv1d_transpose_3_loss: 0.1352\n",
      "Epoch 158/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1478 - conv1d_transpose_1_loss: 0.0142 - conv1d_transpose_3_loss: 0.1336 - val_loss: 0.1504 - val_conv1d_transpose_1_loss: 0.0156 - val_conv1d_transpose_3_loss: 0.1348\n",
      "Epoch 159/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1472 - conv1d_transpose_1_loss: 0.0138 - conv1d_transpose_3_loss: 0.1334 - val_loss: 0.1500 - val_conv1d_transpose_1_loss: 0.0151 - val_conv1d_transpose_3_loss: 0.1349\n",
      "Epoch 160/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1470 - conv1d_transpose_1_loss: 0.0137 - conv1d_transpose_3_loss: 0.1333 - val_loss: 0.1501 - val_conv1d_transpose_1_loss: 0.0155 - val_conv1d_transpose_3_loss: 0.1346\n",
      "Epoch 161/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1468 - conv1d_transpose_1_loss: 0.0136 - conv1d_transpose_3_loss: 0.1332 - val_loss: 0.1502 - val_conv1d_transpose_1_loss: 0.0154 - val_conv1d_transpose_3_loss: 0.1347\n",
      "Epoch 162/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1468 - conv1d_transpose_1_loss: 0.0138 - conv1d_transpose_3_loss: 0.1330 - val_loss: 0.1500 - val_conv1d_transpose_1_loss: 0.0156 - val_conv1d_transpose_3_loss: 0.1344\n",
      "Epoch 163/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1469 - conv1d_transpose_1_loss: 0.0137 - conv1d_transpose_3_loss: 0.1331 - val_loss: 0.1495 - val_conv1d_transpose_1_loss: 0.0154 - val_conv1d_transpose_3_loss: 0.1342\n",
      "Epoch 164/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1466 - conv1d_transpose_1_loss: 0.0136 - conv1d_transpose_3_loss: 0.1330 - val_loss: 0.1494 - val_conv1d_transpose_1_loss: 0.0149 - val_conv1d_transpose_3_loss: 0.1345\n",
      "Epoch 165/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1464 - conv1d_transpose_1_loss: 0.0134 - conv1d_transpose_3_loss: 0.1329 - val_loss: 0.1495 - val_conv1d_transpose_1_loss: 0.0150 - val_conv1d_transpose_3_loss: 0.1345\n",
      "Epoch 166/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1468 - conv1d_transpose_1_loss: 0.0142 - conv1d_transpose_3_loss: 0.1327 - val_loss: 0.1492 - val_conv1d_transpose_1_loss: 0.0155 - val_conv1d_transpose_3_loss: 0.1337\n",
      "Epoch 167/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1467 - conv1d_transpose_1_loss: 0.0143 - conv1d_transpose_3_loss: 0.1325 - val_loss: 0.1494 - val_conv1d_transpose_1_loss: 0.0157 - val_conv1d_transpose_3_loss: 0.1337\n",
      "Epoch 168/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1465 - conv1d_transpose_1_loss: 0.0141 - conv1d_transpose_3_loss: 0.1323 - val_loss: 0.1487 - val_conv1d_transpose_1_loss: 0.0150 - val_conv1d_transpose_3_loss: 0.1338\n",
      "Epoch 169/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1454 - conv1d_transpose_1_loss: 0.0135 - conv1d_transpose_3_loss: 0.1320 - val_loss: 0.1485 - val_conv1d_transpose_1_loss: 0.0154 - val_conv1d_transpose_3_loss: 0.1331\n",
      "Epoch 170/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1454 - conv1d_transpose_1_loss: 0.0135 - conv1d_transpose_3_loss: 0.1319 - val_loss: 0.1490 - val_conv1d_transpose_1_loss: 0.0161 - val_conv1d_transpose_3_loss: 0.1329\n",
      "Epoch 171/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1460 - conv1d_transpose_1_loss: 0.0141 - conv1d_transpose_3_loss: 0.1319 - val_loss: 0.1487 - val_conv1d_transpose_1_loss: 0.0156 - val_conv1d_transpose_3_loss: 0.1331\n",
      "Epoch 172/200\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.1452 - conv1d_transpose_1_loss: 0.0135 - conv1d_transpose_3_loss: 0.1318 - val_loss: 0.1478 - val_conv1d_transpose_1_loss: 0.0149 - val_conv1d_transpose_3_loss: 0.1329\n",
      "Epoch 173/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1449 - conv1d_transpose_1_loss: 0.0135 - conv1d_transpose_3_loss: 0.1315 - val_loss: 0.1476 - val_conv1d_transpose_1_loss: 0.0149 - val_conv1d_transpose_3_loss: 0.1327\n",
      "Epoch 174/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1446 - conv1d_transpose_1_loss: 0.0134 - conv1d_transpose_3_loss: 0.1312 - val_loss: 0.1479 - val_conv1d_transpose_1_loss: 0.0154 - val_conv1d_transpose_3_loss: 0.1324\n",
      "Epoch 175/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1445 - conv1d_transpose_1_loss: 0.0133 - conv1d_transpose_3_loss: 0.1312 - val_loss: 0.1472 - val_conv1d_transpose_1_loss: 0.0146 - val_conv1d_transpose_3_loss: 0.1327\n",
      "Epoch 176/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1446 - conv1d_transpose_1_loss: 0.0134 - conv1d_transpose_3_loss: 0.1312 - val_loss: 0.1473 - val_conv1d_transpose_1_loss: 0.0146 - val_conv1d_transpose_3_loss: 0.1327\n",
      "Epoch 177/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1444 - conv1d_transpose_1_loss: 0.0134 - conv1d_transpose_3_loss: 0.1309 - val_loss: 0.1478 - val_conv1d_transpose_1_loss: 0.0154 - val_conv1d_transpose_3_loss: 0.1324\n",
      "Epoch 178/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1443 - conv1d_transpose_1_loss: 0.0135 - conv1d_transpose_3_loss: 0.1308 - val_loss: 0.1476 - val_conv1d_transpose_1_loss: 0.0150 - val_conv1d_transpose_3_loss: 0.1326\n",
      "Epoch 179/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1444 - conv1d_transpose_1_loss: 0.0137 - conv1d_transpose_3_loss: 0.1307 - val_loss: 0.1480 - val_conv1d_transpose_1_loss: 0.0155 - val_conv1d_transpose_3_loss: 0.1326\n",
      "Epoch 180/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1462 - conv1d_transpose_1_loss: 0.0153 - conv1d_transpose_3_loss: 0.1308 - val_loss: 0.1494 - val_conv1d_transpose_1_loss: 0.0170 - val_conv1d_transpose_3_loss: 0.1324\n",
      "Epoch 181/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1454 - conv1d_transpose_1_loss: 0.0148 - conv1d_transpose_3_loss: 0.1306 - val_loss: 0.1474 - val_conv1d_transpose_1_loss: 0.0158 - val_conv1d_transpose_3_loss: 0.1316\n",
      "Epoch 182/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1444 - conv1d_transpose_1_loss: 0.0139 - conv1d_transpose_3_loss: 0.1305 - val_loss: 0.1474 - val_conv1d_transpose_1_loss: 0.0157 - val_conv1d_transpose_3_loss: 0.1316\n",
      "Epoch 183/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1433 - conv1d_transpose_1_loss: 0.0131 - conv1d_transpose_3_loss: 0.1302 - val_loss: 0.1468 - val_conv1d_transpose_1_loss: 0.0147 - val_conv1d_transpose_3_loss: 0.1321\n",
      "Epoch 184/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1428 - conv1d_transpose_1_loss: 0.0129 - conv1d_transpose_3_loss: 0.1300 - val_loss: 0.1455 - val_conv1d_transpose_1_loss: 0.0143 - val_conv1d_transpose_3_loss: 0.1312\n",
      "Epoch 185/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1428 - conv1d_transpose_1_loss: 0.0129 - conv1d_transpose_3_loss: 0.1299 - val_loss: 0.1455 - val_conv1d_transpose_1_loss: 0.0144 - val_conv1d_transpose_3_loss: 0.1312\n",
      "Epoch 186/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1425 - conv1d_transpose_1_loss: 0.0128 - conv1d_transpose_3_loss: 0.1297 - val_loss: 0.1457 - val_conv1d_transpose_1_loss: 0.0143 - val_conv1d_transpose_3_loss: 0.1314\n",
      "Epoch 187/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1428 - conv1d_transpose_1_loss: 0.0129 - conv1d_transpose_3_loss: 0.1299 - val_loss: 0.1461 - val_conv1d_transpose_1_loss: 0.0143 - val_conv1d_transpose_3_loss: 0.1318\n",
      "Epoch 188/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1427 - conv1d_transpose_1_loss: 0.0129 - conv1d_transpose_3_loss: 0.1298 - val_loss: 0.1460 - val_conv1d_transpose_1_loss: 0.0146 - val_conv1d_transpose_3_loss: 0.1314\n",
      "Epoch 189/200\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.1427 - conv1d_transpose_1_loss: 0.0129 - conv1d_transpose_3_loss: 0.1298 - val_loss: 0.1459 - val_conv1d_transpose_1_loss: 0.0144 - val_conv1d_transpose_3_loss: 0.1316\n",
      "Epoch 190/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1425 - conv1d_transpose_1_loss: 0.0130 - conv1d_transpose_3_loss: 0.1295 - val_loss: 0.1450 - val_conv1d_transpose_1_loss: 0.0143 - val_conv1d_transpose_3_loss: 0.1307\n",
      "Epoch 191/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1425 - conv1d_transpose_1_loss: 0.0132 - conv1d_transpose_3_loss: 0.1293 - val_loss: 0.1455 - val_conv1d_transpose_1_loss: 0.0142 - val_conv1d_transpose_3_loss: 0.1313\n",
      "Epoch 192/200\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.1422 - conv1d_transpose_1_loss: 0.0130 - conv1d_transpose_3_loss: 0.1292 - val_loss: 0.1461 - val_conv1d_transpose_1_loss: 0.0151 - val_conv1d_transpose_3_loss: 0.1309\n",
      "Epoch 193/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1431 - conv1d_transpose_1_loss: 0.0137 - conv1d_transpose_3_loss: 0.1294 - val_loss: 0.1460 - val_conv1d_transpose_1_loss: 0.0150 - val_conv1d_transpose_3_loss: 0.1311\n",
      "Epoch 194/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1421 - conv1d_transpose_1_loss: 0.0130 - conv1d_transpose_3_loss: 0.1291 - val_loss: 0.1447 - val_conv1d_transpose_1_loss: 0.0141 - val_conv1d_transpose_3_loss: 0.1306\n",
      "Epoch 195/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1416 - conv1d_transpose_1_loss: 0.0126 - conv1d_transpose_3_loss: 0.1289 - val_loss: 0.1442 - val_conv1d_transpose_1_loss: 0.0141 - val_conv1d_transpose_3_loss: 0.1302\n",
      "Epoch 196/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1414 - conv1d_transpose_1_loss: 0.0127 - conv1d_transpose_3_loss: 0.1287 - val_loss: 0.1450 - val_conv1d_transpose_1_loss: 0.0144 - val_conv1d_transpose_3_loss: 0.1306\n",
      "Epoch 197/200\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.1419 - conv1d_transpose_1_loss: 0.0132 - conv1d_transpose_3_loss: 0.1286 - val_loss: 0.1478 - val_conv1d_transpose_1_loss: 0.0178 - val_conv1d_transpose_3_loss: 0.1301\n",
      "Epoch 198/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1442 - conv1d_transpose_1_loss: 0.0156 - conv1d_transpose_3_loss: 0.1286 - val_loss: 0.1455 - val_conv1d_transpose_1_loss: 0.0155 - val_conv1d_transpose_3_loss: 0.1300\n",
      "Epoch 199/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1423 - conv1d_transpose_1_loss: 0.0135 - conv1d_transpose_3_loss: 0.1288 - val_loss: 0.1465 - val_conv1d_transpose_1_loss: 0.0162 - val_conv1d_transpose_3_loss: 0.1303\n",
      "Epoch 200/200\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.1428 - conv1d_transpose_1_loss: 0.0141 - conv1d_transpose_3_loss: 0.1287 - val_loss: 0.1453 - val_conv1d_transpose_1_loss: 0.0154 - val_conv1d_transpose_3_loss: 0.1299\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABRiklEQVR4nO3deXwU9f3H8dfsbrK5Q4CcECAgN4gCagFRVOTwqKgtqChQQEUFRdQqtShaLVYFqaXiUQFtLVAUrf3hBRUQoVYQoigUEQPhSAgEyJ3NZnd+fyxZCUdIYJNJNu/n47EPktnZ2c9kEvft95ivYZqmiYiIiEiQsFldgIiIiEggKdyIiIhIUFG4ERERkaCicCMiIiJBReFGREREgorCjYiIiAQVhRsREREJKg6rC6hrXq+Xffv2ER0djWEYVpcjIiIi1WCaJgUFBaSkpGCzVd020+jCzb59+0hNTbW6DBERETkDu3fvpmXLllXu0+jCTXR0NOD74cTExFhcjYiIiFRHfn4+qamp/s/xqjS6cFPRFRUTE6NwIyIi0sBUZ0iJBhSLiIhIUFG4ERERkaCicCMiIiJBpdGNuRERkbPn8Xhwu91WlyFBJjQ09LTTvKtD4UZERKrNNE2ys7M5cuSI1aVIELLZbKSlpREaGnpWx1G4ERGRaqsINgkJCUREROhmqBIwFTfZzcrKolWrVmf1u6VwIyIi1eLxePzBplmzZlaXI0EoPj6effv2UV5eTkhIyBkfRwOKRUSkWirG2ERERFhciQSriu4oj8dzVsdRuBERkRpRV5TUlkD9binciIiISFBRuBEREZGgonAjIiJSQwMGDGDy5MnV3n/nzp0YhkF6enqt1SQ/UbgJkHKPl71HSth9qNjqUkRE5CjDMKp8jBkz5oyOu3TpUn73u99Ve//U1FSysrLo1q3bGb1fdSlE+WgqeIDkFpXR75lPsdsMdvz+KqvLERERICsry//14sWLeeyxx9i2bZt/W3h4eKX93W53taYgN23atEZ12O12kpKSavQaOXNquQkQ29ER3l7TtLgSEZG6Y5omxWXldf4wq/nf2qSkJP8jNjYWwzD835eWltKkSRP+8Y9/MGDAAMLCwvjb3/5Gbm4uN998My1btiQiIoLu3buzcOHCSsc9vluqTZs2/P73v2fs2LFER0fTqlUrXn31Vf/zx7eorFq1CsMw+Pe//03v3r2JiIigb9++lYIXwFNPPUVCQgLR0dGMHz+eRx55hPPOO++MrhWAy+Xi3nvvJSEhgbCwMC6++GLWr1/vf/7w4cOMHDmS+Ph4wsPDad++PfPnzwegrKyMiRMnkpycTFhYGG3atGHGjBlnXEttUstNgNiOzl4zTd8fu6ZKikhjUOL20OWxj+v8fbc8OZiI0MB8hD388MPMnDmT+fPn43Q6KS0tpVevXjz88MPExMSwbNkybrvtNtq2bctFF110yuPMnDmT3/3ud/zmN7/h7bff5q677uKSSy6hU6dOp3zNo48+ysyZM4mPj2fChAmMHTuWtWvXAvDWW2/x9NNP89JLL9GvXz8WLVrEzJkzSUtLO+Nz/fWvf80777zDG2+8QevWrXn22WcZPHgwP/zwA02bNmXatGls2bKFDz/8kObNm/PDDz9QUlICwIsvvsj777/PP/7xD1q1asXu3bvZvXv3GddSmxRuAsRu+ynMeLwmDrvCjYhIQzB58mRuuOGGStsefPBB/9eTJk3io48+YsmSJVWGm6uuuoq7774b8AWmF154gVWrVlUZbp5++mkuvfRSAB555BGuvvpqSktLCQsL409/+hPjxo3jV7/6FQCPPfYYn3zyCYWFhWd0nkVFRcydO5cFCxYwdOhQAF577TWWL1/O66+/zkMPPURmZibnn38+vXv3BnwtUhUyMzNp3749F198MYZh0Lp16zOqoy4o3ASI7dhwY5r6wYpIoxAeYmfLk4Mted9Aqfggr+DxeHjmmWdYvHgxe/fuxeVy4XK5iIyMrPI45557rv/riu6vnJycar8mOTkZgJycHFq1asW2bdv8YanChRdeyKefflqt8zrejh07cLvd9OvXz78tJCSECy+8kK1btwJw1113ceONN7Jx40YGDRrEsGHD6Nu3LwBjxozhyiuvpGPHjgwZMoRrrrmGQYMGnVEttU2fwQFiP6Ybyuu1sBARkTpkGEbAuoescnxomTlzJi+88AKzZ8+me/fuREZGMnnyZMrKyqo8zvEDkQ3DwHuaD4RjX1MxnOHY1xw/xKG6Y41OpuK1JztmxbahQ4eya9culi1bxooVK7jiiiu45557eP755+nZsycZGRl8+OGHrFixguHDhzNw4EDefvvtM66ptmhAcYDYj2u5ERGRhmnNmjVcd9113HrrrfTo0YO2bduyffv2Oq+jY8eOfPnll5W2bdiw4YyPd8455xAaGsrnn3/u3+Z2u9mwYQOdO3f2b4uPj2fMmDH87W9/Y/bs2ZUGRsfExDBixAhee+01Fi9ezDvvvMOhQ4fOuKba0rDjdj1ybBDWjCkRkYbrnHPO4Z133mHdunXExcUxa9YssrOzKwWAujBp0iRuv/12evfuTd++fVm8eDHffPMNbdu2Pe1rj591BdClSxfuuusuHnroIZo2bUqrVq149tlnKS4uZty4cYBvXE+vXr3o2rUrLpeL//u///Of9wsvvEBycjLnnXceNpuNJUuWkJSURJMmTQJ63oFgabj57LPPeO655/jqq6/Iysri3XffZdiwYafcf+nSpcydO5f09HRcLhddu3Zl+vTpDB5c9/29x6vcLaVwIyLSUE2bNo2MjAwGDx5MREQEd9xxB8OGDSMvL69O6xg5ciQ//vgjDz74IKWlpQwfPpwxY8ac0JpzMjfddNMJ2zIyMnjmmWfwer3cdtttFBQU0Lt3bz7++GPi4uIA36rcU6dOZefOnYSHh9O/f38WLVoEQFRUFH/4wx/Yvn07drudCy64gA8++ACbrf51Ahnm2XTgnaUPP/yQtWvX0rNnT2688cbThpvJkyeTkpLCZZddRpMmTZg/fz7PP/88//3vfzn//POr9Z75+fnExsaSl5dHTExMgM7E12eZNvUDAL767UCaRTkDdmwRkfqgtLSUjIwM0tLSCAsLs7qcRunKK68kKSmJv/71r1aXUiuq+h2ryee3pS03Q4cO9U9Hq47Zs2dX+v73v/89//znP/nXv/51ynBTMcq9Qn5+/hnVejq+W3n77nOjMTciInK2iouLefnllxk8eDB2u52FCxeyYsUKli9fbnVp9V79a0uqAa/XS0FBQZW3wZ4xYwaxsbH+R2pqaq3VY/ePdK+1txARkUbCMAw++OAD+vfvT69evfjXv/7FO++8w8CBA60urd5r0AOKZ86cSVFREcOHDz/lPlOnTmXKlCn+7/Pz82st4NhsBnhNDSgWEZGzFh4ezooVK6wuo0FqsOFm4cKFTJ8+nX/+858kJCSccj+n04nTWTfjXypmg3s0oFhERMQyDTLcLF68mHHjxrFkyZJ61Txn1+KZIiIilmtwY24WLlzImDFj+Pvf/87VV19tdTmVVCzBoJYbERER61jaclNYWMgPP/zg/z4jI4P09HT/zYWmTp3K3r17efPNNwFfsBk1ahR//OMf+dnPfkZ2djbg65eMjY215ByOVXGXYrXciIiIWMfSlpsNGzZw/vnn+6dxT5kyhfPPP5/HHnsMgKysLDIzM/37v/LKK5SXl3PPPfeQnJzsf9x3332W1H+8n7qlLC5ERESkEbM03AwYMADTNE94LFiwAIAFCxawatUq//6rVq2qcn+rVSw8pm4pEZHgMmDAACZPnuz/vk2bNifce+14hmHw3nvvnfV7B+o4jUmDG3NTn9mP/jQVbkRE6odrr732lBNP/vOf/2AYBhs3bqzxcdevX88dd9xxtuVVMn36dM4777wTtmdlZdXohrdnYsGCBfVyjagzpXATQJotJSJSv4wbN45PP/2UXbt2nfDcvHnzOO+88+jZs2eNjxsfH09EREQgSjytpKSkOrulSbBQuAkgzZYSEalfrrnmGhISEk4YvlBcXOy/rUhubi4333wzLVu2JCIigu7du7Nw4cIqj3t8t9T27du55JJLCAsLo0uXLiddIuHhhx+mQ4cORERE0LZtW6ZNm4bb7QZ8LSdPPPEEX3/99dHlfAx/zcd3S23evJnLL7+c8PBwmjVrxh133EFhYaH/+TFjxjBs2DCef/55kpOTadasGffcc4//vc5EZmYm1113HVFRUcTExDB8+HD279/vf/7rr7/msssuIzo6mpiYGHr16sWGDRsA2LVrF9deey1xcXFERkbStWtXPvjggzOupToa5H1u6qufZktZXIiISF0xTXAX1/37hkTA0dbyqjgcDkaNGsWCBQt47LHH/GMjlyxZQllZGSNHjqS4uJhevXrx8MMPExMTw7Jly7jtttto27YtF1100Wnfw+v1csMNN9C8eXO++OIL8vPzK43PqRAdHc2CBQtISUlh8+bN3H777URHR/PrX/+aESNG8O233/LRRx/570p8slnAxcXFDBkyhJ/97GesX7+enJwcxo8fz8SJEysFuJUrV5KcnMzKlSv54YcfGDFiBOeddx633377ac/neKZpMmzYMCIjI1m9ejXl5eXcfffdjBgxwj8uduTIkZx//vnMnTsXu91Oeno6ISEhANxzzz2UlZXx2WefERkZyZYtW4iKiqpxHTWhcBNANnVLiUhj4y6G36fU/fv+Zh+ERlZr17Fjx/Lcc8+xatUqLrvsMsDXJXXDDTcQFxdHXFwcDz74oH//SZMm8dFHH7FkyZJqhZsVK1awdetWdu7cScuWLQHfws7Hj5P57W9/6/+6TZs2PPDAAyxevJhf//rXhIeHExUVhcPhICkp6ZTv9dZbb1FSUsKbb75JZKTv/OfMmcO1117LH/7wBxITEwGIi4tjzpw52O12OnXqxNVXX82///3vMwo3K1as4JtvviEjI8O/fNFf//pXunbtyvr167ngggvIzMzkoYceolOnTgC0b9/e//rMzExuvPFGunfvDkDbtm1rXENNqVsqgLT8gohI/dOpUyf69u3LvHnzANixYwdr1qxh7NixAHg8Hp5++mnOPfdcmjVrRlRUFJ988kmlW5FUZevWrbRq1cofbAD69Olzwn5vv/02F198MUlJSURFRTFt2rRqv8ex79WjRw9/sAHo168fXq+Xbdu2+bd17doVu93u/z45OZmcnJwavdex75mamlppXcYuXbrQpEkTtm7dCvhu5TJ+/HgGDhzIM888w44dO/z73nvvvTz11FP069ePxx9/nG+++eaM6qgJtdwEkL9bSuFGRBqLkAhfK4oV71sD48aNY+LEifz5z39m/vz5tG7dmiuuuALwLcL8wgsvMHv2bLp3705kZCSTJ0+mrKysWsc2T9JabxzXZfbFF19w00038cQTTzB48GBiY2NZtGgRM2fOrNF5mKZ5wrFP9p4VXULHPuf1emv0Xqd7z2O3T58+nVtuuYVly5bx4Ycf8vjjj7No0SKuv/56xo8fz+DBg1m2bBmffPIJM2bMYObMmUyaNOmM6qkOtdwEUEW3lEfdUiLSWBiGr3uorh/VGG9zrOHDh2O32/n73//OG2+8wa9+9Sv/B/OaNWu47rrruPXWW+nRowdt27Zl+/bt1T52ly5dyMzMZN++n0Lef/7zn0r7rF27ltatW/Poo4/Su3dv2rdvf8IMrtDQUDwez2nfKz09naKiokrHttlsdOjQodo110TF+e3evdu/bcuWLeTl5dG5c2f/tg4dOnD//ffzySefcMMNNzB//nz/c6mpqUyYMIGlS5fywAMP8Nprr9VKrRUUbgLIrtlSIiL1UlRUFCNGjOA3v/kN+/btY8yYMf7nzjnnHJYvX866devYunUrd955p395n+oYOHAgHTt2ZNSoUXz99desWbOGRx99tNI+55xzDpmZmSxatIgdO3bw4osv8u6771bap02bNv5liA4ePIjL5TrhvUaOHElYWBijR4/m22+/ZeXKlUyaNInbbrvNP97mTHk8HtLT0ys9tmzZwsCBAzn33HMZOXIkGzdu5Msvv2TUqFFceuml9O7dm5KSEiZOnMiqVavYtWsXa9euZf369f7gM3nyZD7++GMyMjLYuHEjn376aaVQVBsUbgKoouVGDTciIvXPuHHjOHz4MAMHDqRVq1b+7dOmTaNnz54MHjyYAQMGkJSUxLBhw6p9XJvNxrvvvovL5eLCCy9k/PjxPP3005X2ue6667j//vuZOHEi5513HuvWrWPatGmV9rnxxhsZMmQIl112GfHx8Sedjh4REcHHH3/MoUOHuOCCC/jFL37BFVdcwZw5c2r2wziJwsJC/5JIFY+rrrrKPxU9Li6OSy65hIEDB9K2bVsWL14MgN1uJzc3l1GjRtGhQweGDx/O0KFDeeKJJwBfaLrnnnvo3LkzQ4YMoWPHjrz00ktnXW9VDPNknYVBLD8/n9jYWPLy8oiJiQnosa/781q+3n2Ev4zqzcAuZ5egRUTqm9LSUjIyMkhLSyMsLMzqciQIVfU7VpPPb7XcBJC9YrZU48qLIiIi9YrCTQBptpSIiIj1FG4CSLOlRERErKdwE0A/3aHY4kJEREQaMYWbAFK3lIg0Bo1sHorUoUD9bincBJBWBReRYFZx19viYgsWypRGoeKu0McuHXEmtPxCAGm2lIgEM7vdTpMmTfxrFEVERJxyKQCRmvJ6vRw4cICIiAgcjrOLJwo3AaRuKREJdhUrVp/pIowiVbHZbLRq1eqsQ7PCTQBpQLGIBDvDMEhOTiYhIQG32211ORJkQkNDsdnOfsSMwk0AaSq4iDQWdrv9rMdFiNQWDSgOIHVLiYiIWE/hJoA0W0pERMR6CjcBVDFbyqtuKREREcso3ARQRcuNwo2IiIh1FG4CyD+g2GtxISIiIo2Ywk0A2Q213IiIiFhN4SaANKBYRETEego3AWQ/+tNUuBEREbGOwk0AqVtKRETEego3AWQo3IiIiFhO4SaA7DbNlhIREbGawk0A2XWfGxEREcsp3ATQT/e5UbgRERGxisJNAGm2lIiIiPUUbgKoouXGVLeUiIiIZRRuAsjfLaVwIyIiYhmFmwDSbCkRERHrKdwEkH+2lMbciIiIWEbhJoDULSUiImI9hZsAqpgtpfvciIiIWEfhJoAqWm7ULSUiImIdhZsA+qlbyuJCREREGjGFmwDSgGIRERHrKdwEkM2m5RdERESspnATQHZDC2eKiIhYTeEmgI423CjciIiIWEjhJoDULSUiImI9hZsAsmu2lIiIiOUUbgJIs6VERESsZ2m4+eyzz7j22mtJSUnBMAzee++9075m9erV9OrVi7CwMNq2bcvLL79c+4VWk7qlRERErGdpuCkqKqJHjx7MmTOnWvtnZGRw1VVX0b9/fzZt2sRvfvMb7r33Xt55551arrR6NKBYRETEeg4r33zo0KEMHTq02vu//PLLtGrVitmzZwPQuXNnNmzYwPPPP8+NN95YS1VWn6aCi4iIWK9Bjbn5z3/+w6BBgyptGzx4MBs2bMDtdp/0NS6Xi/z8/EqP2qJuKREREes1qHCTnZ1NYmJipW2JiYmUl5dz8ODBk75mxowZxMbG+h+pqam1Vp9mS4mIiFivQYUbAONogKhgHu0COn57halTp5KXl+d/7N69u9Zq02wpERER61k65qamkpKSyM7OrrQtJycHh8NBs2bNTvoap9OJ0+msi/IwNKBYRETEcg2q5aZPnz4sX7680rZPPvmE3r17ExISYlFVP7FrzI2IiIjlLA03hYWFpKenk56eDvimeqenp5OZmQn4upRGjRrl33/ChAns2rWLKVOmsHXrVubNm8frr7/Ogw8+aEX5J9BsKREREetZ2i21YcMGLrvsMv/3U6ZMAWD06NEsWLCArKwsf9ABSEtL44MPPuD+++/nz3/+MykpKbz44ov1Yho4aLaUiIhIfWBpuBkwYIB/QPDJLFiw4IRtl156KRs3bqzFqs6cf0Cxso2IiIhlGtSYm/pOdygWERGxnsJNANkMdUuJiIhYTeEmgHSfGxEREesp3ASQv+VG3VIiIiKWUbgJoJ/uc2NxISIiIo2Ywk0AVYSbqmaAiYiISO1SuAmgitlS6pYSERGxjsJNAGm2lIiIiPUUbgJIs6VERESsp3ATQJotJSIiYj2FmwD6qeXG4kJEREQaMYWbALJpVXARERHLKdwEkO3oT1PdUiIiItZRuAkgu1Fxnxvd60ZERMQqCjcBVDHmBjQdXERExCoKNwFkOzbcqOVGRETEEgo3AVQxoBh8XVMiIiJS9xRuAshuqFtKRETEago3AWQ75qepbikRERFrKNwE0LEtN1qCQURExBoKNwGk2VIiIiLWU7gJIOPYlhtlGxEREUso3ASYf30pjbkRERGxhMJNgFWMu1G3lIiIiDUUbgLMv76Uwo2IiIglFG4CzK6VwUVERCylcBNgNv+YG4sLERERaaQUbgLMpjE3IiIillK4CTDNlhIREbGWwk2AqeVGRETEWgo3AWbXbCkRERFLKdwEmGZLiYiIWEvhJsAMQ7OlRERErKRwE2AVA4rVLSUiImINhZsA02wpERERayncBNjRbKOWGxEREYso3ASYv+VG4UZERMQSCjcBZtOAYhEREUsp3ASY/yZ+GnMjIiJiCYWbAFO3lIiIiLUUbgLMpqngIiIillK4CTB7xWwpdUuJiIhYQuEmwCrG3JgKNyIiIpZQuAmwn7qlLC5ERESkkVK4CTC7ZkuJiIhYSuEmwDRbSkRExFoKNwGm2VIiIiLWUrgJsIq1pbRwpoiIiDUUbgLMbmhVcBERESsp3ASYZkuJiIhYS+EmwDRbSkRExFqWh5uXXnqJtLQ0wsLC6NWrF2vWrKly/7feeosePXoQERFBcnIyv/rVr8jNza2jak9Ps6VERESsZWm4Wbx4MZMnT+bRRx9l06ZN9O/fn6FDh5KZmXnS/T///HNGjRrFuHHj+O6771iyZAnr169n/PjxdVz5qVV0S2nMjYiIiDUsDTezZs1i3LhxjB8/ns6dOzN79mxSU1OZO3fuSff/4osvaNOmDffeey9paWlcfPHF3HnnnWzYsOGU7+FyucjPz6/0qE0Vs6U0FVxERMQaloWbsrIyvvrqKwYNGlRp+6BBg1i3bt1JX9O3b1/27NnDBx98gGma7N+/n7fffpurr776lO8zY8YMYmNj/Y/U1NSAnsfxNFtKRETEWpaFm4MHD+LxeEhMTKy0PTExkezs7JO+pm/fvrz11luMGDGC0NBQkpKSaNKkCX/6059O+T5Tp04lLy/P/9i9e3dAz+N4mi0lIiJiLcsHFBtHWzoqmKZ5wrYKW7Zs4d577+Wxxx7jq6++4qOPPiIjI4MJEyac8vhOp5OYmJhKj9qklhsRERFrOax64+bNm2O3209opcnJyTmhNafCjBkz6NevHw899BAA5557LpGRkfTv35+nnnqK5OTkWq/7dLT8goiIiLUsa7kJDQ2lV69eLF++vNL25cuX07dv35O+pri4GJutcsl2ux3wtfjUB1p+QURExFqWdktNmTKFv/zlL8ybN4+tW7dy//33k5mZ6e9mmjp1KqNGjfLvf+2117J06VLmzp3Ljz/+yNq1a7n33nu58MILSUlJseo0KtF9bkRERKxlWbcUwIgRI8jNzeXJJ58kKyuLbt268cEHH9C6dWsAsrKyKt3zZsyYMRQUFDBnzhweeOABmjRpwuWXX84f/vAHq07hBDbdoVhERMRShllf+nPqSH5+PrGxseTl5dXK4OLf/d8WXv88gwmXtuORoZ0CfnwREZHGqCaf35bPlgo2dt2hWERExFIKNwFWMYtdY25ERESsoXATYFoVXERExFoKNwGm2VIiIiLWUrgJMM2WEhERsZbCTYDZtbaUiIiIpRRuAqziDsWNbIa9iIhIvaFwE2BaW0pERMRaCjcBptlSIiIi1rJ0+YWgUpoH//kzF+7KBS7XbCkRERGLKNwEirsEVv+BHtiAy/Eo24iIiFjijLqlysvLWbFiBa+88goFBQUA7Nu3j8LCwoAW16DYQwGw4cWOR8sviIiIWKTGLTe7du1iyJAhZGZm4nK5uPLKK4mOjubZZ5+ltLSUl19+uTbqrP8cTv+XobjVLSUiImKRGrfc3HffffTu3ZvDhw8THh7u33799dfz73//O6DFNSj2Y8NNuWZLiYiIWKTGLTeff/45a9euJTQ0tNL21q1bs3fv3oAV1uDYHWDYwPT6Wm7ULSUiImKJGrfceL1ePB7PCdv37NlDdHR0QIpqsI623jgNtdyIiIhYpcbh5sorr2T27Nn+7w3DoLCwkMcff5yrrroqkLU1PA5fa1Yobs2WEhERsUiNu6VeeOEFLrvsMrp06UJpaSm33HIL27dvp3nz5ixcuLA2amw4jrbchFKu5RdEREQsUuNwk5KSQnp6OgsXLmTjxo14vV7GjRvHyJEjKw0wbpQcFeHGrW4pERERi5zRTfzCw8MZO3YsY8eODXQ9DZv9mG4phRsRERFL1DjcvPnmm1U+P2rUqDMupsGraLkxynGrW0pERMQSNQ439913X6Xv3W43xcXFhIaGEhER0bjDzTEtN6VquREREbFEjWdLHT58uNKjsLCQbdu2cfHFF2tA8dGWGyflKNuIiIhY44zWljpe+/bteeaZZ05o1Wl0jmm50U38RERErBGQcANgt9vZt29foA7XMPnDjW7iJyIiYpUaj7l5//33K31vmiZZWVnMmTOHfv36BaywBsk/oFizpURERKxS43AzbNiwSt8bhkF8fDyXX345M2fODFRdDdMxLTfqlhIREbFGjcON1+utjTqCwzE38VPDjYiIiDUCNuZGqNxyo3QjIiJiiWq13EyZMqXaB5w1a9YZF9PgHTvmRt1SIiIilqhWuNm0aVO1DmYYxlkV0+DZf7rPjQYUi4iIWKNa4WblypW1XUdwcBxznxuFGxEREUtozE0g2TWgWERExGpntCr4+vXrWbJkCZmZmZSVlVV6bunSpQEprEFyHHMTP425ERERsUSNW24WLVpEv3792LJlC++++y5ut5stW7bw6aefEhsbWxs1Nhz2nwYUq1tKRETEGjUON7///e954YUX+L//+z9CQ0P54x//yNatWxk+fDitWrWqjRobDv99btRyIyIiYpUah5sdO3Zw9dVXA+B0OikqKsIwDO6//35effXVgBfYoByzcKZmS4mIiFijxuGmadOmFBQUANCiRQu+/fZbAI4cOUJxcXFgq2tojmm5UbeUiIiINaodbtLT0wHo378/y5cvB2D48OHcd9993H777dx8881cccUVtVJkg3G05cZpaLaUiIiIVao9W6pnz56cf/75DBs2jJtvvhmAqVOnEhISwueff84NN9zAtGnTaq3QBkFjbkRERCxX7ZabtWvX0rNnT55//nnatWvHrbfeyurVq/n1r3/N+++/z6xZs4iLi6vNWuu/Y+9zo6YbERERS1Q73PTp04fXXnuN7Oxs5s6dy549exg4cCDt2rXj6aefZs+ePbVZZ8Og+9yIiIhYrsYDisPDwxk9ejSrVq3i+++/5+abb+aVV14hLS2Nq666qjZqbDiOabkxTTAVcEREROrcWS2/0K5dOx555BEeffRRYmJi+PjjjwNVV8NU0XJjlANoULGIiIgFzmj5BYDVq1czb9483nnnHex2O8OHD2fcuHGBrK3hOablBsDjNbHbGvlK6SIiInWsRuFm9+7dLFiwgAULFpCRkUHfvn3505/+xPDhw4mMjKytGhuOY2ZLAXjVLSUiIlLnqh1urrzySlauXEl8fDyjRo1i7NixdOzYsTZra3iOuUMxoLsUi4iIWKDa4SY8PJx33nmHa665BrvdXps1NVxHW26cRjlgasaUiIiIBaodbt5///3arCM4HG25AQjBg+m1sBYREZFG6qxmS8lxjrbcwNHFM9VyIyIiUucsDzcvvfQSaWlphIWF0atXL9asWVPl/i6Xi0cffZTWrVvjdDpp164d8+bNq6NqT8N+XLjRmBsREZE6d8ZTwQNh8eLFTJ48mZdeeol+/frxyiuvMHToULZs2UKrVq1O+prhw4ezf/9+Xn/9dc455xxycnIoLy+v48pPwWYDmwO85b6VwdVyIyIiUucsDTezZs1i3LhxjB8/HoDZs2fz8ccfM3fuXGbMmHHC/h999BGrV6/mxx9/pGnTpgC0adOmLks+PbvTF24MtdyIiIhYwbJuqbKyMr766isGDRpUafugQYNYt27dSV/z/vvv07t3b5599llatGhBhw4dePDBBykpKTnl+7hcLvLz8ys9atUx60up5UZERKTuWdZyc/DgQTweD4mJiZW2JyYmkp2dfdLX/Pjjj3z++eeEhYXx7rvvcvDgQe6++24OHTp0ynE3M2bM4Iknngh4/ad0dNyNEzdezZYSERGpc5YPKDaMyssTmKZ5wrYKXq8XwzB46623uPDCC7nqqquYNWsWCxYsOGXrzdSpU8nLy/M/du/eHfBzqEQrg4uIiFjKspab5s2bY7fbT2ilycnJOaE1p0JycjItWrQgNjbWv61z586YpsmePXto3779Ca9xOp04nc4TtteaY9aX0pgbERGRumdZy01oaCi9evVi+fLllbYvX76cvn37nvQ1/fr1Y9++fRQWFvq3ff/999hsNlq2bFmr9VZbxfpShsbciIiIWMHSbqkpU6bwl7/8hXnz5rF161buv/9+MjMzmTBhAuDrUho1apR//1tuuYVmzZrxq1/9ii1btvDZZ5/x0EMPMXbsWMLDw606jcqOWV9KLTciIiJ1z9Kp4CNGjCA3N5cnn3ySrKwsunXrxgcffEDr1q0ByMrKIjMz079/VFQUy5cvZ9KkSfTu3ZtmzZoxfPhwnnrqKatO4UTHrAyulhsREZG6Z5hm4/oEzs/PJzY2lry8PGJiYgL/Bm/8HDJWc2/ZPdx+98N0bxl7+teIiIhIlWry+W35bKmgc7RbymlobSkRERErKNwE2jHdUhpzIyIiUvcUbgLtmAHFGnMjIiJS9xRuAu3YAcVquREREalzCjeBdkzLjdujcCMiIlLXFG4C7Zib+BWUui0uRkREpPFRuAm0Y1pu8hVuRERE6pzCTaAdM+Ymr0ThRkREpK4p3ATa0YUznbjJLym3uBgREZHGR+Em0BxHu6WMcnVLiYiIWEDhJtDsFd1SbnVLiYiIWEDhJtAqWm4oJ1/hRkREpM4p3ASaWm5EREQspXATaI6fwk1+qQYUi4iI1DWFm0Cz/zSgWC03IiIidU/hJtCObblRuBEREalzCjeBdrTlxkk5rnIvpW6PxQWJiIg0Lgo3gXZMyw2ge92IiIjUMYWbQDs6WyrM5htMrLsUi4iI1C2Fm0A7ep8bp+ELNRpULCIiUrcUbgLN/tPCmaBuKRERkbqmcBNo/jsUHx1zo5YbERGROqVwE2hHW25CTIUbERERKyjcBNrR2VJ2PBh4dZdiERGROqZwE2hH73MDvnE3GlAsIiJStxRuAu1oyw2AU3cpFhERqXMKN4GmlhsRERFLKdwEmmH8tHgmbk0FFxERqWMKN7Wh4l43hlstNyIiInVM4aY2+O91U67lF0REROqYwk1tsP+0eKa6pUREROqWwk1tqNRy48brNS0uSEREpPFQuKkNjjAAIgwXXhOKytQ1JSIiUlcUbmpDXBsA2tuzAa0MLiIiUpcUbmpDYlcAuofsAdCgYhERkTqkcFMbEroA0IlMAA0qFhERqUMKN7UhsRsAaWYmBl51S4mIiNQhhZva0OwcsIcSbpbQ0jig9aVERETqkMJNbbA7IL4jAJ2NTLXciIiI1CGFm9pytGuqo7GbQ0VlFhcjIiLSeCjc1JajM6Y62TLZmHnY4mJEREQaD4Wb2lIxY8rYzcZdRygp81hckIiISOOgcFNbjnZLtbHtx+YpYcOuQxYXJCIi0jgo3NSWqASIaI4dL+2NvazbkWt1RSIiIo2Cwk1tMQxIPNo1Zctk3Q8HLS5IRESkcVC4qU1J5wLQ1/Ydm/fmaUq4iIhIHVC4qU1drwdgiH0D4WYJX/yorikREZHapnBTm1r0gmbnEI6LIbb1/EfjbkRERGqdwk1tMgzocRMAN9o/46Nvs3F7vBYXJSIiEtwUbmrbub5w09e+BXv+bpZ9k2VxQSIiIsFN4aa2NUmFNv0BuM6+llc/+xHTNC0uSkREJHhZHm5eeukl0tLSCAsLo1evXqxZs6Zar1u7di0Oh4PzzjuvdgsMhB43AzDGsZzMrGyNvREREalFloabxYsXM3nyZB599FE2bdpE//79GTp0KJmZmVW+Li8vj1GjRnHFFVfUUaVnqduNEJdGgnGYex3v8uqaH62uSEREJGhZGm5mzZrFuHHjGD9+PJ07d2b27NmkpqYyd+7cKl935513csstt9CnT5/TvofL5SI/P7/So86FhMFVzwEw1v4he7/fxCffZdd9HSIiIo2AZeGmrKyMr776ikGDBlXaPmjQINatW3fK182fP58dO3bw+OOPV+t9ZsyYQWxsrP+Rmpp6VnWfsfZXQsercRhengqZxyNvbyIrr8SaWkRERIKYZeHm4MGDeDweEhMTK21PTEwkO/vkrRrbt2/nkUce4a233sLhcFTrfaZOnUpeXp7/sXv37rOu/YwNmYHpCOci2/+40/1X7l+cjserwcUiIiKBZPmAYsMwKn1vmuYJ2wA8Hg+33HILTzzxBB06dKj28Z1OJzExMZUelolrjXHdHADudCyj1a53eGjJ1wo4IiIiAWRZuGnevDl2u/2EVpqcnJwTWnMACgoK2LBhAxMnTsThcOBwOHjyySf5+uuvcTgcfPrpp3VV+tnp/gu49BEAnnbMo/zrJTzwj3TKdXM/ERGRgLAs3ISGhtKrVy+WL19eafvy5cvp27fvCfvHxMSwefNm0tPT/Y8JEybQsWNH0tPTueiii+qq9LM34BE4dwQhhocXQ+eQ9u2LjF/wJYeLyqyuTEREpMGr3sCVWjJlyhRuu+02evfuTZ8+fXj11VfJzMxkwoQJgG+8zN69e3nzzTex2Wx069at0usTEhIICws7YXu9Zxgw7GWIToK1f+Q+x1KW79zJLS/exzO3DqBHahOrKxQREWmwLA03I0aMIDc3lyeffJKsrCy6devGBx98QOvWrQHIyso67T1vGiybDa58Epp3wPuv+7mSjXQtvZ+HX76L8y69jomXn4PTYbe6ShERkQbHMBvZWgD5+fnExsaSl5dn7eDiY2V9g2fJGOyHdgCw3NOTRTHjGHntlVzWMeGkA6xFREQak5p8fls+W0qA5HOx3/kZXHA7XsPOlfaNvFI4iX1/u4s7537IpszDVlcoIiLSYKjlpr458D3ujx8j5IcPASg0w3i5/Fp2d/oV9w3pQdv4KIsLFBERqXs1+fxWuKmvdq6l7MPfELo/HYBsM45ZnuE4e93CpIGdSIgOs7Y+ERGROqRwU4UGE24AvF74binujx8npHAPAFu9rfgjt9Dh4hu449J2RDktHRMuIiJSJxRuqtCgwk0Fdyl8+Srlq5/DUeZb+PM7b2vecNxI+0tv5dY+bQgP1cwqEREJXgo3VWiQ4aZC8SHMNbPwfPk6Dk8xAF96O/LH0PFcN2Qov+jZEptNM6tERCT4KNxUoUGHmwrFh/D+Zy7edX/C4SnBaxos9FzOR4njeej6vpzbsonVFYqIiASUwk0VgiLcVMjbi+eTadi/eweAI2Yks8p/Sfn5o3lgSFeaRTktLlBERCQwFG6qEFThpsLOtbiXPUTIge8A2OpN5Xn7eG64fgRXn5tscXEiIiJnTzfxa2za9CPkrjVw9UzKnU3obNvN6+bj5PzjPu7/2zryStxWVygiIlJnFG6Chc0OF4zHcd8mPOePBuBXjo+Z+P1Y7vrjYr7dm2dxgSIiInVD4SbYRDTFft2LMPIdyiISaWfL4k8ljzB97hss+ybL6upERERqncJNsGo/kNC711Ke2INmRgFv2p9i8eI3+HCzAo6IiAQ3hZtgFhWPY+wHmO2uIMJw8YpjJm8sWsgn32VbXZmIiEitUbgJds4ojJsXYrYbSLhRxmuOZ5m7cAn/3rrf6spERERqhcJNY+BwYoz4K2brfkQbJcyzP8MLf3uPlf/LsboyERGRgFO4aSxCIzBuWYzZojdxRiHzHU8z42//Yv3OQ1ZXJiIiElAKN42JMxrj1rcxE7sRb+Qx3/40v1nwId/vL7C6MhERkYBRuGlswuMwbnsPb9P2tDBymeP9PRNf/5R9R0qsrkxERCQgFG4ao6h4bKOW4o1MpKNtD78r/T3jX/+cvGLdyVhERBo+hZvGqkkrbLctxRsazUW2/zHpyB+4443/Uur2WF2ZiIjIWVG4acySumG7+e94baEMta/n6r2zuW/hRjzeRrWWqoiIBBmFm8Yu7RJsN7yCicEox3I6bnuZx/75LY1ssXgREQkiCjcC3W7AGPoHAKaEvE3YhrnM+fQHi4sSERE5Mwo34nPRnXDZbwGYFvIW+z6dy6IvMy0uSkREpOYUbuQnlzwI/SYD8LRjHv/951xWbNEyDSIi0rAo3MhPDAMGTse84HZshsnzjrn8c+FcPt9+0OrKREREqk3hRiozDIyhz+LtcQt2w2Sm7UUWvvln1mw/YHVlIiIi1aJwIyey2bBdNwdPlxsINTzMtv2RJW/O4aNvs62uTERE5LQUbuTkbHbsN76Gp9svCTE8zLK9yLqFv2fuyh80TVxEROo1hRs5NbsD+w2v4O0xEofh5cmQN0j49D4e+PsX5JdqqQYREamfFG6kajY7tmF/hkFP4zXs3Gj/nDu3jWfi7L+RvvuI1dWJiIicQOFGTs8woO9EbKPewx3enI62Pbxa8hD/fOVxXl61Ha+WaxARkXpE4UaqL+0SQu75AnfbgYQZbh53LKDnpyN55NW3ycwttro6ERERQOFGaioqnpBbl2AOfQ63PYILbdv4XdYE3pk9mT99slWriouIiOUUbqTmbDaMi+4gZNKXFLW6HKdRzv32xQz8fDiPPT+Llf/TXY1FRMQ6htnI5vXm5+cTGxtLXl4eMTExVpfT8Jkm5jf/wL3s14SWHQHgO29rVje/ifMGj6FPh2QMw7C2RhERafBq8vmtcCOBUXSQss9egPWvE+otAeCAGcu/I4YSf9ldDOjdA7tNIUdERM6Mwk0VFG5qWfEh8j6bi/HVPGLcvjWp3KadtY6LKG7/c84d8AtaJsVbXKSIiDQ0CjdVULipIx43BZveJe+zP9MyP92/udQMYVNoL/LaDKVVn+vpnNZK3VYiInJaCjdVULipeyWZ6WSuXkCTnR+R6Mnyb3ebdjbYunM4+WJadOhJpx4/wxnXwsJKRUSkvlK4qYLCjYVMk0M/biTri3/QZOdHtHDvPGGXTOc5FLYZTMIFN9K8XU/fDQRFRKTRU7ipgsJN/eHK/h97/rOEkp0biMz7gdbmXmzGT7+OWUY8mbEX4GjZi4SOF5LStjv2yDgLKxYREaso3FRB4aZ+Mk2TLT/sYM8X7xKbuZweZZsIN8pO2C/PiCUnsgNlCd2JaHU+yZ1+RlhCO7DZLahaRETqisJNFRRuGoaCgjx2rv+Qgh1fEHHga5JcO0kyDp10XzcODock4gmPJzzMSXhkNKEtemC06AnN2kFMCoTF1vEZiIhIICncVEHhpmHyeE12Zh1gz/Z0indtJPTAZhIL/8c55i7CDPdpX18aFk9xYm8cLXsSlZiGLSoeXAVQmgdN20KLXuBw1sGZiIjImVC4qYLCTXDJyStkx44f2J+5jYP797H3cBHugoN0MzLobsughXGQJkbRaY9TbnOS1+w8IjoMILxdX2jeAaKTNaBZRKSeULipgsJN8HN7vOQUuNh7uIStWflsz8zCkbOZ5IKvSSrNINE4RFPyKSScQjOczrZdxBv5Jx7H5qTUGY83Mh4jMgEjOgFHTBKhsYnYY5J9LT5N0yAk3IKzFBFpXBRuqqBw07i5PV52Hypmx4EifjxQyM7cIkrLPDQr3UV09hekFaXT3fiRVOMADsNbvWOGROMNb44R1RxHVLyvyyuyOUQl+gJQs3YQ2wrsjlo+OxGR4KVwUwWFG6lKTkEp3+7NY/eBPPL2Z1Ccuw9XXjZhpblElh8mznuY5kYeScYh2hjZxBrF1Tqux3BQFt0Ko2kbQpu1xtYk1Rd4YltCk1SISlL4ERGpQoMKNy+99BLPPfccWVlZdO3aldmzZ9O/f/+T7rt06VLmzp1Leno6LpeLrl27Mn36dAYPHlzt91O4kbNR7vFS5PKw+3AxX+w4yNaMTEqO7MdbkIOtJJem5NGUApoZeSQaR2hjZNPGyD7toGcvdkrCE3FHpWCPbIozJISQ8CiMuDRf15czBkIjoFl7iNVdnEWk8Wkw4Wbx4sXcdtttvPTSS/Tr149XXnmFv/zlL2zZsoVWrVqdsP/kyZNJSUnhsssuo0mTJsyfP5/nn3+e//73v5x//vnVek+FG6ktHq9JbpGLAwUuDhWVkVtYRm5RGQfySziUlYE7ZzvhRXtI4gAtjFxSyKWFcYBk4xAhhqfa71McnoKreRfCYhMJi43HiGjm6wZr1h4SOoMzqhbPUkTEGg0m3Fx00UX07NmTuXPn+rd17tyZYcOGMWPGjGodo2vXrowYMYLHHnusWvsr3IiVvF6T3KIy9ueXsj+/lOz8UnLyiinO3Yc3bzeO/D14SvIpLSsjmhJaG9m0NA4QZZQSRQlpRhZ2o+o/2VJnM4hMIKRJEvboJIhOgmbnQPOOEN9B9/wRkQapJp/flnXyl5WV8dVXX/HII49U2j5o0CDWrVtXrWN4vV4KCgpo2rTpKfdxuVy4XC7/9/n5J86KEakrNptBfLST+Ggn3VocGzI6V9rPVe4hJ991NAS52JZfSk5+KYcPHyImN53Q/EyM0kPEUUCcUUACR+hg20OicYQwVy64cuHQ1pPWUBoWj6dpe8JTumCr6OKyh0KLnrrfj4gEBcvCzcGDB/F4PCQmJlbanpiYSHZ2drWOMXPmTIqKihg+fPgp95kxYwZPPPHEWdUqUtecDjupTSNIbRpxkmf7Ab4AtPtQMTsPFvP9oWL+U+ii+EgOhQcyKcjdS1TZIeKNI6QYubQz9tHOto8k4zBhpQdg3wHYd+L/RHjsTsrj2mOP74Cj1QXQdZjvDs8iIg2I5dMzjONukmaa5gnbTmbhwoVMnz6df/7znyQkJJxyv6lTpzJlyhT/9/n5+aSmpp55wSL1hNNh55yEaM5JiD5mayfA93d0pNjN3iMlZOWVsiOvhM+OlHA49yC2Q9uxHdxOqnc3zcnDxCDaKKa3bRvxnnzsB7+Fg9/C1qV4P/4NB2K7Y0/sSpPUzjji2/u6uOLagCPUkvMWETkdy8JN8+bNsdvtJ7TS5OTknNCac7zFixczbtw4lixZwsCBA6vc1+l04nSqmV0aF8MwiIsMJS4y9LjuL4D+eLwmOw4U8r/sAnYeLOK/ucW8lVeC/UgG0YU/0rI8k8vtm7jQto3EvG8g7xv4/qcjeLFRGN4Cd9J5NLnoVuztB2oqu4jUG5b91yg0NJRevXqxfPlyrr/+ev/25cuXc911153ydQsXLmTs2LEsXLiQq6++ui5KFQk6dptBh8RoOiRGH/fMzwAoKHWzNauAv277DteOz/Ee3EFS+R7SjCzSjGyijFJiSnZDxm7I+BdFthiKY9JwJrQjKukcbE3TfON3mnfQEhYiUufqxVTwl19+mT59+vDqq6/y2muv8d1339G6dWumTp3K3r17efPNNwFfsBk1ahR//OMfueGGG/zHCQ8PJza2ejNANFtKpOZM02TP4RJ25hax62ARudmZlO3fRkr2pwwx19DMKDjp6wojU/GkXUZEag9Cmrf76Ymk7r7p6yIi1dRgpoKD7yZ+zz77LFlZWXTr1o0XXniBSy65BIAxY8awc+dOVq1aBcCAAQNYvXr1CccYPXo0CxYsqNb7KdyIBE65x8vGH/ezbfN/yd39PZ7cDJK92bS1ZXG+sR2nUX7S15kYuOLPxdGuP46WPX3jeAy7b52upm3V2iMiJ2hQ4aauKdyI1B6P12RXbhFbswr4LmMP3h0raX5kM23NTFoaB/BgI5Ry2tmyTnmM0rB4zLaXEda8NUZIOIQ1gagEaNIaEruBzVZ3JyQi9YbCTRUUbkTqlmmaHCh08UNOIZv35PHtvnzycnbT8tAXdPZs41zbjyQbhzAwiaa4yqUq3GHNcKX2J7RpS0IimmCEN/HdlND0QvEhX4tP2wEQ30mtPyJBRuGmCgo3IvWDaZrsPVLCt3vz2XO4mEJXOQePFODOWEfLvA3EUEQ4ZcQaRcQbR+hg7CHKKK3WsUsiUjDjO+Fs1gq7Kx8O7QAMSD4Xks+DlPMgoSuEhNXmKYpIACncVEHhRqT+K3V7yC0q41BhGdv2F7Ax8zAZ2YdJyvuatJJvCffkE00xMUYx0RRjYnCEKGIopo9tC87TLFQK4DHslEakYDZpTZjNi704B8NdAs5ozLAmmDEpmDEp2GNTfYuVugrh4Dbfv/EdIaELJHaB8Lg6+ImIiMJNFRRuRBq+4rJyDhaUcaCwlJx8F/mlbopcHg4VlbHnQC7h2V9hz88k3nuQQjOMnWYSdrx0s2XQ3cigmy3jlDO8aqosIhEi43GEhmMLjYCQCF+LUEgEhEb6QlDL3r4B0wX7AAOat4eYlmc+fih3B5Qc9k23V/ebNBINYm0pEZEzFRHqoFUzB62anWx5CoC+eL0mOQUuCkrduMq9uMq9lJV7ySspY/GBQg5m7cKVswOOZFJQBvvNOIpxEmWU0JQCkoxcUoxDJBu5pBi5lJhOtpstKCaM9sYeOtp209I4SGjxfijeX+NzMLGBAZgmBr7/x/Q4Iih3NsEd2oTy0Fg8zhgiw8MIC3VCRDPfI2M17FzjO0hid+g/BZLOhchm4IytOjCZJuxaB1++CqYHfnY3tO5b49pF6ju13IhIo1dW7iW/1E1JmYcQuw27zcBh87WI7D5czA85heSVuCkr9+L2eCnzmBSUutm7Pwfv/v/hKTlCiNdFGGWEG2W+f3HRxCiku5FBd1sG5djYb8YRgodWxn5CDc8Z1+vBhpsQwnBV2u417JQ74zBDIsCwYRgGhmEDw8Bmd2DzuDAOZ1R6TW5sNxyRcUSE2AgJiwJn9E+PqETfPYkqBmh7Pb5Q5PX4WqXCYn3bTRPKCqEwB4oOQOF+39c2B8Sm+pbraNpWM93krKhbqgoKNyISaKZpUuAqp7TMg8c0cbm95JW4yStxc6TETV5xGUeKfV8fKXZTUFyCWXiA/FI3+aUeMAzsNhtRRilNjUJijUKaUEhYeQFHiktwmB6aGgXEG3ns9sbzD88AinEyzvEB19i+IN7II9ooqVatpWYIb3suwYbJL+yrzypkeR3hmCER2Fz5GN6qxzl5Q6Mxks/FCI8DR5ivFSomxTdmyWb3ddvZ7L5AFNkcIhOgvNQXlMpdviBV8XDGgCvfN0PO9EJohO8eSSERPz20HEjQUbipgsKNiDQkxWXl/JBTSE6+i4OFLmxH1w0LC7Hhcns5UuJmW3Y+P2TlQtEhnO7DGO4Syso9eDweML2Uez14PF5seNlhS6Ndm9akNY8krHAfCYc3kJ3v4lCRmwjDRSQlRBklxFBMC+MgXW07aWHk+uvxmAZebIScJBQVm04OmLEcJJaDZiwOPKQYubQxsgk3yuryx4ZpC8EMicAIjfDdLykkEhxOX0uTYQOO/utwHh0fdTQUOaqYQedw+lqzIpv79rOH+lqsSvN8rVchYb7w1aSVb0xVSLhvH48L3CW+rytau07G6wF3MZTmQ0E25HwHh3dCYldIG+DreqwvvL7fLewhdfaWGnMjIhIkIkIdnNuyyVkfp7isnMPFbppFhhIWYj+6tTswGIAiVzn7jpSwL6+UUrfHt7J8STlL8ko4kFdEoctDUZmXwjIPxWUeykuLCHcdBHcR2a4wDpuRlNnCiQpzEBnqIMrpINJpJyoshJJSF65939HWu4sIw0U4LpoZ+SQZh4imGDte7PjCV6hRTjPyiTeOUEYI+804XIT4Z8fFUEyE4aLYdHKIaLymQbjh6wYMx4Xd8P3/uuF1Y7jywJV31j+7QDLtoZih0RimB7zlvpDgLQdvuX/s1Ulfh4HXEeHLZg4nRkRTX+tWQieI7+wLVhXdhqbpa/UqzYeyIl+Y8reOOSC8iS+AhTWB0iO+wenFh3xfe8sBw9eyltTdNzMwrIkvqBUf8oWtb9+Bbxb5Ats5A33jtgqyIX+vrzUuvpPvkdLTsq5ItdyIiMhZMU0Tt8ckxG5gnKJVoqzcS8bBIsrKvZR7vRSXeSgodZNfWk5+iZuC0nIKSsvJL3VTUOr73u3xYprgNU1MwGsCponpLafIDUUuDxFOOy2ahBMd5iC/2E1RSQmuknzKSorwuIqJwEUYLv9YKAPT/7Bh4sTtD1zhuHAa7lNGjAhcNDfyaEoBTsONEzeFZhj5ROLFIIwymhoFtDQOkmAcOeOfp9u0c5ho/udNZZ/ZjB62HXS27T7j41mhzB5B6G/3BXQ2n1puRESkzhiGQaij6g+xUIeNjknHr0Jfuzxek/xjxj4Vucop95qUe7yUe008XhO3x4vHa/q/P/a5KKeDmPAQbAa4yr24PSbF5V72l7rJPFTMnsMllHt9ASwi1EHzqFAMwyAnv5RDBcV43KV4yssocNvIczvAU0qU+zBRRgnl2CnHjgc7btP3bzk2SgmllFDsNhvdUmLokdqEL13l5Odmk593iAP5LhxmGXEUkGQcopNtN+cYe7HhxcSGBxseDMoIocCMoJgwwMSOFwcebHhpZhTQ0jhANMUcIYojZhRHiCLPjMSNHQNINnLpYuyitbEfh+EFwGsa5BLDem9H/uEZQI7ZhMH2DXQ0dpNlNiXLbEqycYhzjL2YRhiXWHibAoUbEREJSnabb3xSXGSo1aX4VbRymZh4vb5WKc9JOlBC7bZjug9/4vGa7DtSwo8Hi9h9qJj8UjfppeV4TBPT9B3fW9Hadcz3Jkf/PbrdW7HdBI/X679dQpPwEJKbhJGPwWf5pWTnlZCXn09RYT6HPOGUeG2Eh9iJiQmhVdMImna5mviUWHbvOszuzMPsd9jYGR5C8ygnl9TFD/QUFG5ERETqSHVauapitxmkNo0gtemp7vFkjV6t69edunXTAREREQkqCjciIiISVBRuREREJKgo3IiIiEhQUbgRERGRoKJwIyIiIkFF4UZERESCisKNiIiIBBWFGxEREQkqCjciIiISVBRuREREJKgo3IiIiEhQUbgRERGRoKJwIyIiIkHFYXUBdc00TQDy8/MtrkRERESqq+Jzu+JzvCqNLtwUFBQAkJqaanElIiIiUlMFBQXExsZWuY9hVicCBRGv18u+ffuIjo7GMIyAHjs/P5/U1FR2795NTExMQI9dXwT7OQb7+YHOMRgE+/lB8J9jsJ8fBP4cTdOkoKCAlJQUbLaqR9U0upYbm81Gy5Yta/U9YmJigvaXtUKwn2Ownx/oHINBsJ8fBP85Bvv5QWDP8XQtNhU0oFhERESCisKNiIiIBBWFmwByOp08/vjjOJ1Oq0upNcF+jsF+fqBzDAbBfn4Q/OcY7OcH1p5joxtQLCIiIsFNLTciIiISVBRuREREJKgo3IiIiEhQUbgRERGRoKJwEyAvvfQSaWlphIWF0atXL9asWWN1SWdsxowZXHDBBURHR5OQkMCwYcPYtm1bpX3GjBmDYRiVHj/72c8sqrhmpk+ffkLtSUlJ/udN02T69OmkpKQQHh7OgAED+O677yysuObatGlzwjkahsE999wDNMzr99lnn3HttdeSkpKCYRi89957lZ6vznVzuVxMmjSJ5s2bExkZyc9//nP27NlTh2dxalWdn9vt5uGHH6Z79+5ERkaSkpLCqFGj2LdvX6VjDBgw4ITretNNN9XxmZza6a5hdX4v6/M1hNOf48n+Lg3D4LnnnvPvU5+vY3U+H+rD36LCTQAsXryYyZMn8+ijj7Jp0yb69+/P0KFDyczMtLq0M7J69WruuecevvjiC5YvX055eTmDBg2iqKio0n5DhgwhKyvL//jggw8sqrjmunbtWqn2zZs3+5979tlnmTVrFnPmzGH9+vUkJSVx5ZVX+tclawjWr19f6fyWL18OwC9/+Uv/Pg3t+hUVFdGjRw/mzJlz0uerc90mT57Mu+++y6JFi/j8888pLCzkmmuuwePx1NVpnFJV51dcXMzGjRuZNm0aGzduZOnSpXz//ff8/Oc/P2Hf22+/vdJ1feWVV+qi/Go53TWE0/9e1udrCKc/x2PPLSsri3nz5mEYBjfeeGOl/errdazO50O9+Fs05axdeOGF5oQJEypt69Spk/nII49YVFFg5eTkmIC5evVq/7bRo0eb1113nXVFnYXHH3/c7NGjx0mf83q9ZlJSkvnMM8/4t5WWlpqxsbHmyy+/XEcVBt59991ntmvXzvR6vaZpNuzrZ5qmCZjvvvuu//vqXLcjR46YISEh5qJFi/z77N2717TZbOZHH31UZ7VXx/HndzJffvmlCZi7du3yb7v00kvN++67r3aLC5CTnePpfi8b0jU0zepdx+uuu868/PLLK21rSNfx+M+H+vK3qJabs1RWVsZXX33FoEGDKm0fNGgQ69ats6iqwMrLywOgadOmlbavWrWKhIQEOnTowO23305OTo4V5Z2R7du3k5KSQlpaGjfddBM//vgjABkZGWRnZ1e6nk6nk0svvbTBXs+ysjL+9re/MXbs2EqLxTbk63e86ly3r776CrfbXWmflJQUunXr1iCvbV5eHoZh0KRJk0rb33rrLZo3b07Xrl158MEHG1SLI1T9exls13D//v0sW7aMcePGnfBcQ7mOx38+1Je/xUa3cGagHTx4EI/HQ2JiYqXtiYmJZGdnW1RV4JimyZQpU7j44ovp1q2bf/vQoUP55S9/SevWrcnIyGDatGlcfvnlfPXVV/X+jpsXXXQRb775Jh06dGD//v089dRT9O3bl++++85/zU52PXft2mVFuWftvffe48iRI4wZM8a/rSFfv5OpznXLzs4mNDSUuLi4E/ZpaH+rpaWlPPLII9xyyy2VFiQcOXIkaWlpJCUl8e233zJ16lS+/vprf7dkfXe638tguoYAb7zxBtHR0dxwww2VtjeU63iyz4f68reocBMgx/4fMfgu+vHbGqKJEyfyzTff8Pnnn1faPmLECP/X3bp1o3fv3rRu3Zply5ad8Ida3wwdOtT/dffu3enTpw/t2rXjjTfe8A9eDKbr+frrrzN06FBSUlL82xry9avKmVy3hnZt3W43N910E16vl5deeqnSc7fffrv/627dutG+fXt69+7Nxo0b6dmzZ12XWmNn+nvZ0K5hhXnz5jFy5EjCwsIqbW8o1/FUnw9g/d+iuqXOUvPmzbHb7SekzZycnBOSa0MzadIk3n//fVauXEnLli2r3Dc5OZnWrVuzffv2OqoucCIjI+nevTvbt2/3z5oKluu5a9cuVqxYwfjx46vcryFfP6Ba1y0pKYmysjIOHz58yn3qO7fbzfDhw8nIyGD58uWVWm1OpmfPnoSEhDTY63r872UwXMMKa9asYdu2baf924T6eR1P9flQX/4WFW7OUmhoKL169TqhuXD58uX07dvXoqrOjmmaTJw4kaVLl/Lpp5+SlpZ22tfk5uaye/dukpOT66DCwHK5XGzdupXk5GR/U/Cx17OsrIzVq1c3yOs5f/58EhISuPrqq6vcryFfP6Ba161Xr16EhIRU2icrK4tvv/22QVzbimCzfft2VqxYQbNmzU77mu+++w63291gr+vxv5cN/Roe6/XXX6dXr1706NHjtPvWp+t4us+HevO3GJBhyY3cokWLzJCQEPP11183t2zZYk6ePNmMjIw0d+7caXVpZ+Suu+4yY2NjzVWrVplZWVn+R3FxsWmapllQUGA+8MAD5rp168yMjAxz5cqVZp8+fcwWLVqY+fn5Fld/eg888IC5atUq88cffzS/+OIL85prrjGjo6P91+uZZ54xY2NjzaVLl5qbN282b775ZjM5OblBnNuxPB6P2apVK/Phhx+utL2hXr+CggJz06ZN5qZNm0zAnDVrlrlp0yb/bKHqXLcJEyaYLVu2NFesWGFu3LjRvPzyy80ePXqY5eXlVp2WX1Xn53a7zZ///Odmy5YtzfT09Ep/ly6XyzRN0/zhhx/MJ554wly/fr2ZkZFhLlu2zOzUqZN5/vnn14vzM82qz7G6v5f1+Rqa5ul/T03TNPPy8syIiAhz7ty5J7y+vl/H030+mGb9+FtUuAmQP//5z2br1q3N0NBQs2fPnpWmTTc0wEkf8+fPN03TNIuLi81BgwaZ8fHxZkhIiNmqVStz9OjRZmZmprWFV9OIESPM5ORkMyQkxExJSTFvuOEG87vvvvM/7/V6zccff9xMSkoynU6neckll5ibN2+2sOIz8/HHH5uAuW3btkrbG+r1W7ly5Ul/L0ePHm2aZvWuW0lJiTlx4kSzadOmZnh4uHnNNdfUm/Ou6vwyMjJO+Xe5cuVK0zRNMzMz07zkkkvMpk2bmqGhoWa7du3Me++918zNzbX2xI5R1TlW9/eyPl9D0zz976lpmuYrr7xihoeHm0eOHDnh9fX9Op7u88E068ffonG0WBEREZGgoDE3IiIiElQUbkRERCSoKNyIiIhIUFG4ERERkaCicCMiIiJBReFGREREgorCjYiIiAQVhRsREREJKgo3IiL4VjF+7733rC5DRAJA4UZELDdmzBgMwzjhMWTIEKtLE5EGyGF1ASIiAEOGDGH+/PmVtjmdTouqEZGGTC03IlIvOJ1OkpKSKj3i4uIAX5fR3LlzGTp0KOHh4aSlpbFkyZJKr9+8eTOXX3454eHhNGvWjDvuuIPCwsJK+8ybN4+uXbvidDpJTk5m4sSJlZ4/ePAg119/PREREbRv357333+/dk9aRGqFwo2INAjTpk3jxhtv5Ouvv+bWW2/l5ptvZuvWrQAUFxczZMgQ4uLiWL9+PUuWLGHFihWVwsvcuXO55557uOOOO9i8eTPvv/8+55xzTqX3eOKJJxg+fDjffPMNV111FSNHjuTQoUN1ep4iEgABW19cROQMjR492rTb7WZkZGSlx5NPPmmapmkC5oQJEyq95qKLLjLvuusu0zRN89VXXzXj4uLMwsJC//PLli0zbTabmZ2dbZqmaaakpJiPPvroKWsAzN/+9rf+7wsLC03DMMwPP/wwYOcpInVDY25EpF647LLLmDt3bqVtTZs29X/dp0+fSs/16dOH9PR0ALZu3UqPHj2IjIz0P9+vXz+8Xi/btm3DMAz27dvHFVdcUWUN5557rv/ryMhIoqOjycnJOdNTEhGLKNyISL0QGRl5QjfR6RiGAYBpmv6vT7ZPeHh4tY4XEhJywmu9Xm+NahIR62nMjYg0CF988cUJ33fq1AmALl26kJ6eTlFRkf/5tWvXYrPZ6NChA9HR0bRp04Z///vfdVqziFhDLTciUi+4XC6ys7MrbXM4HDRv3hyAJUuW0Lt3by6++GLeeustvvzyS15//XUARo4cyeOPP87o0aOZPn06Bw4cYNKkSdx2220kJiYCMH36dCZMmEBCQgJDhw6loKCAtWvXMmnSpLo9URGpdQo3IlIvfPTRRyQnJ1fa1rFjR/73v/8BvplMixYt4u677yYpKYm33nqLLl26ABAREcHHH3/MfffdxwUXXEBERAQ33ngjs2bN8h9r9OjRlJaW8sILL/Dggw/SvHlzfvGLX9TdCYpInTFM0zStLkJEpCqGYfDuu+8ybNgwq0sRkQZAY25EREQkqCjciIiISFDRmBsRqffUey4iNaGWGxEREQkqCjciIiISVBRuREREJKgo3IiIiEhQUbgRERGRoKJwIyIiIkFF4UZERESCisKNiIiIBJX/B+bvTQ72+Ml1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 40, 6)]           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 40, 32)            608       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 40, 32)            3104      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,712\n",
      "Trainable params: 3,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "mask_pred_model = MaskPredModel(x_train)\n",
    "# Define early stopping callback to monitor validation loss and stop if it doesn't improve for 5 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with 20 epochs and batch size of 32, using the early stopping callback\n",
    "history = mask_pred_model.fit(x_train_mask, [x_train,train_mask], epochs=200, batch_size=128, validation_data=(x_test_mask, [x_test,test_mask]), callbacks=[early_stop])\n",
    "\n",
    "# Plot the training history for loss and accuracy\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "encoder = Model(inputs=mask_pred_model.input,outputs=mask_pred_model.layers[3].output)\n",
    "encoder.compile()\n",
    "encoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune the AE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "Y_ohe = ohe.fit_transform(Y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FineTunedModel(nn_model,x_train,y_train,X_sc,Y_ohe,method='hybrid'):\n",
    "    # nn_model.compile()\n",
    "\n",
    "    from keras.layers import BatchNormalization\n",
    "    from keras.models import clone_and_build_model\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "    if method == 'hybrid':\n",
    "        model_copy= clone_and_build_model(nn_model)\n",
    "        model_copy.set_weights(weights=nn_model.get_weights())\n",
    "        intermediate_model = Model(inputs=model_copy.input,outputs=model_copy.layers[-2].output)\n",
    "    else:\n",
    "        intermediate_model = clone_and_build_model(nn_model)\n",
    "        intermediate_model.set_weights(weights=nn_model.get_weights())\n",
    "\n",
    "    for l in intermediate_model.layers:\n",
    "        l.trainable=False\n",
    "\n",
    "    fine_tuned_layers = Dense(units=64,activation='relu')(intermediate_model.output)\n",
    "    output_layer = Dense(units=Y_ohe.shape[1],activation='softmax')(fine_tuned_layers)\n",
    "\n",
    "    # Define the model\n",
    "    fine_tuned_model = Model(inputs=intermediate_model.input, outputs=output_layer)\n",
    "\n",
    "    # Compile the model with binary cross-entropy loss function and Adam optimizer\n",
    "    fine_tuned_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #categorical_hinge\n",
    "\n",
    "    #train the model with freezed layer\n",
    "    fine_tuned_model.fit(x_train, y_train, epochs=500, batch_size=int(len(x_train) * 0.2), validation_data=(X_sc[::50], Y_ohe[::50]), callbacks=[early_stop], verbose=0)\n",
    "\n",
    "    for l in fine_tuned_model.layers:\n",
    "        l.trainable=True\n",
    "\n",
    "    # Create a custom Adam optimizer with a small learning rate\n",
    "    custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "    # Compiling again is necessary to update the trainable parameter before training\n",
    "    fine_tuned_model.compile(loss='categorical_crossentropy', optimizer=custom_optimizer, metrics=['accuracy']) #categorical_hinge\n",
    "\n",
    "    #train the model with freezed layer\n",
    "    fine_tuned_model.fit(x_train, y_train, epochs=50, batch_size=int(len(x_train) * 0.2), validation_data=(X_sc[::50], Y_ohe[::50]), callbacks=[early_stop], verbose=0)\n",
    "\n",
    "    return fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuned Model Training\n",
      "**************** repetation - 0 ****************\n",
      "Repetition 1: Training samples per class = 4\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8378306869575847\n",
      "Repetition 1: Training samples per class = 8\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8563268530438931\n",
      "Repetition 1: Training samples per class = 16\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8887438914959922\n",
      "Repetition 1: Training samples per class = 32\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9396948698406398\n",
      "Repetition 1: Training samples per class = 64\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9479947994799479\n",
      "Repetition 1: Training samples per class = 90\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9477834684620676\n",
      "**************** repetation - 1 ****************\n",
      "Repetition 2: Training samples per class = 4\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.7981390955095179\n",
      "Repetition 2: Training samples per class = 8\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8525641592585919\n",
      "Repetition 2: Training samples per class = 16\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8887707099761222\n",
      "Repetition 2: Training samples per class = 32\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9537505006202902\n",
      "Repetition 2: Training samples per class = 64\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9618923775719639\n",
      "Repetition 2: Training samples per class = 90\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.967971174056651\n",
      "**************** repetation - 2 ****************\n",
      "Repetition 3: Training samples per class = 4\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.7820866703050335\n",
      "Repetition 3: Training samples per class = 8\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8512440207028587\n",
      "Repetition 3: Training samples per class = 16\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9139272580319255\n",
      "Repetition 3: Training samples per class = 32\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9459661569132841\n",
      "Repetition 3: Training samples per class = 64\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9659605383615284\n",
      "Repetition 3: Training samples per class = 90\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9859909918927035\n",
      "**************** repetation - 3 ****************\n",
      "Repetition 4: Training samples per class = 4\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8203176368765146\n",
      "Repetition 4: Training samples per class = 8\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.7768816807379773\n",
      "Repetition 4: Training samples per class = 16\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8921753439461243\n",
      "Repetition 4: Training samples per class = 32\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9439830827707217\n",
      "Repetition 4: Training samples per class = 64\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9316065078993667\n",
      "Repetition 4: Training samples per class = 90\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9799727583341028\n",
      "**************** repetation - 4 ****************\n",
      "Repetition 5: Training samples per class = 4\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8166825710703304\n",
      "Repetition 5: Training samples per class = 8\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8433231626404822\n",
      "Repetition 5: Training samples per class = 16\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8891694253411903\n",
      "Repetition 5: Training samples per class = 32\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "F1 score on whole dataset = 0.9477959872738703\n",
      "Repetition 5: Training samples per class = 64\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9399218117300453\n",
      "Repetition 5: Training samples per class = 90\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9639020988378675\n",
      "**************** repetation - 5 ****************\n",
      "Repetition 6: Training samples per class = 4\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.7561746885769813\n",
      "Repetition 6: Training samples per class = 8\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8471897888648474\n",
      "Repetition 6: Training samples per class = 16\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8960496228052623\n",
      "Repetition 6: Training samples per class = 32\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9516891653500599\n",
      "Repetition 6: Training samples per class = 64\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9479178378334987\n",
      "Repetition 6: Training samples per class = 90\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "F1 score on whole dataset = 0.965802237728153\n",
      "**************** repetation - 6 ****************\n",
      "Repetition 7: Training samples per class = 4\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8448518827430826\n",
      "Repetition 7: Training samples per class = 8\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8574365768036518\n",
      "Repetition 7: Training samples per class = 16\n",
      "16/16 [==============================] - 0s 4ms/step\n",
      "F1 score on whole dataset = 0.892536831002688\n",
      "Repetition 7: Training samples per class = 32\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "F1 score on whole dataset = 0.9341195429628163\n",
      "Repetition 7: Training samples per class = 64\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "F1 score on whole dataset = 0.9236451045229878\n",
      "Repetition 7: Training samples per class = 90\n",
      "16/16 [==============================] - 0s 4ms/step\n",
      "F1 score on whole dataset = 0.9779520917678812\n",
      "**************** repetation - 7 ****************\n",
      "Repetition 8: Training samples per class = 4\n",
      "16/16 [==============================] - 0s 8ms/step\n",
      "F1 score on whole dataset = 0.8022423418495673\n",
      "Repetition 8: Training samples per class = 8\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.880229981064353\n",
      "Repetition 8: Training samples per class = 16\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.8699843477743677\n",
      "Repetition 8: Training samples per class = 32\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9396938629649977\n",
      "Repetition 8: Training samples per class = 64\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9255355832209158\n",
      "Repetition 8: Training samples per class = 90\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.9879747373233563\n",
      "**************** repetation - 8 ****************\n",
      "Repetition 9: Training samples per class = 4\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "F1 score on whole dataset = 0.7950821298436428\n",
      "Repetition 9: Training samples per class = 8\n",
      "16/16 [==============================] - 0s 7ms/step\n",
      "F1 score on whole dataset = 0.8663495154140136\n",
      "Repetition 9: Training samples per class = 16\n",
      "16/16 [==============================] - 0s 7ms/step\n",
      "F1 score on whole dataset = 0.9040645702319949\n",
      "Repetition 9: Training samples per class = 32\n",
      "16/16 [==============================] - 0s 7ms/step\n",
      "F1 score on whole dataset = 0.9419439062753815\n",
      "Repetition 9: Training samples per class = 64\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "F1 score on whole dataset = 0.943948717948718\n",
      "Repetition 9: Training samples per class = 90\n",
      "16/16 [==============================] - 0s 8ms/step\n",
      "F1 score on whole dataset = 0.9598508091559232\n",
      "**************** repetation - 9 ****************\n",
      "Repetition 10: Training samples per class = 4\n",
      "16/16 [==============================] - 0s 8ms/step\n",
      "F1 score on whole dataset = 0.8042555929994386\n",
      "Repetition 10: Training samples per class = 8\n",
      "16/16 [==============================] - 0s 15ms/step\n",
      "F1 score on whole dataset = 0.8575801876939014\n",
      "Repetition 10: Training samples per class = 16\n",
      "16/16 [==============================] - 0s 7ms/step\n",
      "F1 score on whole dataset = 0.8835871238583415\n",
      "Repetition 10: Training samples per class = 32\n",
      "16/16 [==============================] - 0s 6ms/step\n",
      "F1 score on whole dataset = 0.9518008929124647\n",
      "Repetition 10: Training samples per class = 64\n",
      "16/16 [==============================] - 0s 8ms/step\n",
      "F1 score on whole dataset = 0.9418817835215878\n",
      "Repetition 10: Training samples per class = 90\n",
      "16/16 [==============================] - 0s 6ms/step\n",
      "F1 score on whole dataset = 0.957872602018897\n",
      "F1 Scores Matrix:\n",
      "[[0.83783069 0.85632685 0.88874389 0.93969487 0.9479948  0.94778347]\n",
      " [0.7981391  0.85256416 0.88877071 0.9537505  0.96189238 0.96797117]\n",
      " [0.78208667 0.85124402 0.91392726 0.94596616 0.96596054 0.98599099]\n",
      " [0.82031764 0.77688168 0.89217534 0.94398308 0.93160651 0.97997276]\n",
      " [0.81668257 0.84332316 0.88916943 0.94779599 0.93992181 0.9639021 ]\n",
      " [0.75617469 0.84718979 0.89604962 0.95168917 0.94791784 0.96580224]\n",
      " [0.84485188 0.85743658 0.89253683 0.93411954 0.9236451  0.97795209]\n",
      " [0.80224234 0.88022998 0.86998435 0.93969386 0.92553558 0.98797474]\n",
      " [0.79508213 0.86634952 0.90406457 0.94194391 0.94394872 0.95985081]\n",
      " [0.80425559 0.85758019 0.88358712 0.95180089 0.94188178 0.9578726 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "set_of_samples = [4, 8, 16, 32, 64, 90]\n",
    "num_repetitions = 10\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "print(\"FineTuned Model Training\")\n",
    "F1_score_finetuned = np.zeros((num_repetitions, len(set_of_samples)))\n",
    "\n",
    "for repetition in range(num_repetitions):\n",
    "    print(f\"**************** repetation - {repetition} ****************\")\n",
    "    for i, samples in enumerate(set_of_samples):\n",
    "        print(f'Repetition {repetition + 1}: Training samples per class = {samples}')\n",
    " \n",
    "        x_val, y_val, _ = Resample(X_sc, Y, Z, samples)\n",
    "        y_val = ohe.transform(y_val.reshape(-1, 1))\n",
    "\n",
    "        fine_tuned_model = FineTunedModel(encoder,x_val,y_val,X_sc,Y_ohe,method='sota2')\n",
    "        \n",
    "        x_train,y_train,z_train = Resample(X,Y,Z,50)\n",
    "        x_train = sc.transform(x_train.reshape(-1,x_train.shape[2])).reshape(x_train.shape)\n",
    "        \n",
    "        y_pred = fine_tuned_model.predict(x_train)\n",
    "        y_pred = ohe.inverse_transform(y_pred)\n",
    "        f1 = f1_score(y_train, y_pred, average='macro')\n",
    "        print(f'F1 score on whole dataset = {f1}')\n",
    "        F1_score_finetuned[repetition, i] = f1\n",
    "\n",
    "print(\"F1 Scores Matrix:\")\n",
    "print(F1_score_finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the experiment in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f1_scores_ae = np.mean(F1_score_finetuned, axis=0)\n",
    "variance_f1_scores_ae= np.var(F1_score_finetuned, axis=0)\n",
    "\n",
    "results_dict = {\n",
    "    'No training samples': set_of_samples,\n",
    "    'Model': ['SOA2'] * len(set_of_samples) ,\n",
    "    'Mean F1 score': mean_f1_scores_ae,\n",
    "    'Error': variance_f1_scores_ae\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('SOA2_mask_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The proposed hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propose hybrid method\n",
      "Repetition 1: Training samples per class = 4\n",
      "F1 score on whole dataset = 0.8465246256118677\n",
      "Repetition 1: Training samples per class = 8\n",
      "F1 score on whole dataset = 0.870915360624599\n",
      "Repetition 1: Training samples per class = 16\n",
      "F1 score on whole dataset = 0.8728452753044804\n",
      "Repetition 1: Training samples per class = 32\n",
      "F1 score on whole dataset = 0.890880816395274\n",
      "Repetition 1: Training samples per class = 64\n",
      "F1 score on whole dataset = 0.9021268156819208\n",
      "Repetition 1: Training samples per class = 90\n",
      "F1 score on whole dataset = 0.9123999979784452\n",
      "Repetition 2: Training samples per class = 4\n",
      "F1 score on whole dataset = 0.8289303141262172\n",
      "Repetition 2: Training samples per class = 8\n",
      "F1 score on whole dataset = 0.8590626186545322\n",
      "Repetition 2: Training samples per class = 16\n",
      "F1 score on whole dataset = 0.8852700309168309\n",
      "Repetition 2: Training samples per class = 32\n",
      "F1 score on whole dataset = 0.9063814256411964\n",
      "Repetition 2: Training samples per class = 64\n",
      "F1 score on whole dataset = 0.8999285961732518\n",
      "Repetition 2: Training samples per class = 90\n",
      "F1 score on whole dataset = 0.8976706441281307\n",
      "Repetition 3: Training samples per class = 4\n",
      "F1 score on whole dataset = 0.8384511742415253\n",
      "Repetition 3: Training samples per class = 8\n",
      "F1 score on whole dataset = 0.8586438242192065\n",
      "Repetition 3: Training samples per class = 16\n",
      "F1 score on whole dataset = 0.8822024009828227\n",
      "Repetition 3: Training samples per class = 32\n",
      "F1 score on whole dataset = 0.8999307226745618\n",
      "Repetition 3: Training samples per class = 64\n",
      "F1 score on whole dataset = 0.8965061836551493\n",
      "Repetition 3: Training samples per class = 90\n",
      "F1 score on whole dataset = 0.9021072127322262\n",
      "Repetition 4: Training samples per class = 4\n",
      "F1 score on whole dataset = 0.8270204799604833\n",
      "Repetition 4: Training samples per class = 8\n",
      "F1 score on whole dataset = 0.8590051849080721\n",
      "Repetition 4: Training samples per class = 16\n",
      "F1 score on whole dataset = 0.8832244218371998\n",
      "Repetition 4: Training samples per class = 32\n",
      "F1 score on whole dataset = 0.8976898427146607\n",
      "Repetition 4: Training samples per class = 64\n",
      "F1 score on whole dataset = 0.9065597685294475\n",
      "Repetition 4: Training samples per class = 90\n",
      "F1 score on whole dataset = 0.8942795125535496\n",
      "Repetition 5: Training samples per class = 4\n",
      "F1 score on whole dataset = 0.8517118170185929\n",
      "Repetition 5: Training samples per class = 8\n",
      "F1 score on whole dataset = 0.8628809904509183\n",
      "Repetition 5: Training samples per class = 16\n",
      "F1 score on whole dataset = 0.8826953911642537\n",
      "Repetition 5: Training samples per class = 32\n",
      "F1 score on whole dataset = 0.8922816963982388\n",
      "Repetition 5: Training samples per class = 64\n",
      "F1 score on whole dataset = 0.906286379603985\n",
      "Repetition 5: Training samples per class = 90\n",
      "F1 score on whole dataset = 0.9024217036965453\n",
      "F1 Scores Matrix:\n",
      "[[0.84652463 0.87091536 0.87284528 0.89088082 0.90212682 0.9124    ]\n",
      " [0.82893031 0.85906262 0.88527003 0.90638143 0.8999286  0.89767064]\n",
      " [0.83845117 0.85864382 0.8822024  0.89993072 0.89650618 0.90210721]\n",
      " [0.82702048 0.85900518 0.88322442 0.89768984 0.90655977 0.89427951]\n",
      " [0.85171182 0.86288099 0.88269539 0.8922817  0.90628638 0.9024217 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create fullySupervided NEtwork\n",
    "print(\"Propose hybrid method\")\n",
    "F1_score_hybrid = np.zeros((num_repetitions, len(set_of_samples)))\n",
    "\n",
    "for repetition in range(num_repetitions):\n",
    "    for i, samples in enumerate(set_of_samples):\n",
    "        print(f'Repetition {repetition + 1}: Training samples per class = {samples}')\n",
    "        x_val, y_val, _ = resample(X_sc, Y,Z, samples)\n",
    "        y_val_ohe = ohe.transform(y_val.reshape(-1, 1))\n",
    "\n",
    "        hybrid_model = FineTunedModel(psuedo_label_model,x_val,y_val_ohe,X_sc,Y_ohe)\n",
    "        \n",
    "        x_train,y_train,z_train = resample(X,Y,Z,50)\n",
    "        x_train = sc.transform(x_train.reshape(-1,x_train.shape[2])).reshape(x_train.shape)\n",
    "        \n",
    "        y_pred = hybrid_model.predict(x_train)\n",
    "        y_pred = ohe.inverse_transform(y_pred)\n",
    "        y_pred = Cascade_Hybrid_FDI(x_train,z_train,y_pred)\n",
    "        f1 = f1_score(y_train, y_pred, average='macro')\n",
    "        print(f'F1 score on whole dataset = {f1}')\n",
    "        F1_score_hybrid[repetition, i] = f1\n",
    "\n",
    "print(\"F1 Scores Matrix:\")\n",
    "print(F1_score_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "set_of_samples = [4, 8, 16, 32, 64, 90]\n",
    "\n",
    "# Calculate the mean and variance of F1 scores for each sample size\n",
    "\n",
    "\n",
    "mean_f1_scores_hybrid = np.mean(F1_score_hybrid, axis=0)\n",
    "variance_f1_scores_hybrid = np.var(F1_score_hybrid, axis=0)\n",
    "\n",
    "mean_f1_scores_ae = np.mean(F1_score_finetuned, axis=0)\n",
    "variance_f1_scores_ae= np.var(F1_score_finetuned, axis=0)\n",
    "\n",
    "# Set the width of each bar\n",
    "bar_width = 0.25\n",
    "\n",
    "# Create an array representing the x-axis positions\n",
    "x = np.arange(len(set_of_samples))\n",
    "\n",
    "# Choose colors for three classes\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# Create side-by-side bar plots with error bars\n",
    "plt.bar(x, mean_f1_scores_hybrid, bar_width, yerr=np.sqrt(variance_f1_scores_hybrid), capsize=3, color=colors[1], label='hybrid')\n",
    "plt.bar(x + bar_width, mean_f1_scores_ae, bar_width, yerr=np.sqrt(variance_f1_scores_ae), capsize=3, color=colors[2], label='SOAT2')\n",
    "\n",
    "plt.grid(visible=False)\n",
    "plt.legend()\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.ylim([0.5,1])\n",
    "plt.title('F1 Score vs. Sample Size with Variance')\n",
    "plt.xticks(x, [str(x) for x in set_of_samples])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
